\documentclass[authoryear]{elsarticle}

% ------------ packages -------------

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{sidecap}

\usepackage{url}
\usepackage[bookmarks]{hyperref}

%\usetikzlibrary{shapes.misc,fit}
\usetikzlibrary{%
   arrows,%
   calc,%
   fit,%
   patterns,%
   plotmarks,%
   shapes.geometric,%
   shapes.misc,%
   shapes.symbols,%
   shapes.arrows,%
   shapes.callouts,%
   shapes.multipart,%
   shapes.gates.logic.US,%
   shapes.gates.logic.IEC,%
   er,%
   automata,%
   backgrounds,%
   chains,%
   topaths,%
   trees,%
   petri,%
   mindmap,%
   matrix,%
   calendar,%
   folding,%
   fadings,%
   through,%
   patterns,%
   positioning,%
   scopes,%
   decorations.fractals,%
   decorations.shapes,%
   decorations.text,%
   decorations.pathmorphing,%
   decorations.pathreplacing,%
   decorations.footprints,%
   decorations.markings,%
   shadows}

%\usepackage{hyperref}
%\usepackage[bookmarks]{hyperref}
%\usepackage[colorlinks=true,citecolor=red,linkcolor=black]{hyperref}

% ------------ custom defs -------------

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\dd}{\,\mathrm{d}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\Tsys}{T_\text{sys}}

\newcommand{\Rsys}{R_\text{sys}}
\newcommand{\lRsys}{\ul{R}_\text{sys}}
\newcommand{\uRsys}{\ol{R}_\text{sys}}

\newcommand{\fsys}{f_\text{sys}}
\newcommand{\Fsys}{F_\text{sys}}
\newcommand{\lFsys}{\ul{F}_\text{sys}}
\newcommand{\uFsys}{\ol{F}_\text{sys}}

\newcommand{\lgt}{\ul{g}}
\newcommand{\ugt}{\ol{g}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\wei}{\operatorname{Wei}} % Weibull Distribution
\newcommand{\ig}{\operatorname{IG}}   % Inverse Gamma Distribution

\newcommand{\El}{\ul{\operatorname{E}}}
\newcommand{\Eu}{\ol{\operatorname{E}}}

\def\yz{y\uz}
\def\yn{y\un}
%\def\yi{y\ui}
\newcommand{\yfun}[1]{y^{({#1})}}
\newcommand{\yfunl}[1]{\ul{y}^{({#1})}}
\newcommand{\yfunu}[1]{\ol{y}^{({#1})}}

\def\ykz{y\uz_k}
\def\ykn{y\un_k}

\def\yzl{\ul{y}\uz}
\def\yzu{\ol{y}\uz}
\def\ynl{\ul{y}\un}
\def\ynu{\ol{y}\un}
\def\yil{\ul{y}\ui}
\def\yiu{\ol{y}\ui}

\def\ykzl{\ul{y}\uz_k}
\def\ykzu{\ol{y}\uz_k}
\def\yknl{\ul{y}\un_k}
\def\yknu{\ol{y}\un_k}

\newcommand{\ykzfun}[1]{y\uz_{#1}}
\newcommand{\ykzlfun}[1]{\ul{y}\uz_{#1}}
\newcommand{\ykzufun}[1]{\ol{y}\uz_{#1}}


\def\nz{n\uz}
\def\nn{n\un}
%\def\ni{n\ui}
\newcommand{\nfun}[1]{n^{({#1})}}
\newcommand{\nfunl}[1]{\ul{n}^{({#1})}}
\newcommand{\nfunu}[1]{\ol{n}^{({#1})}}

\def\nkz{n\uz_k}
\def\nkn{n\un_k}
\newcommand{\nkzfun}[1]{n\uz_{#1}}
\newcommand{\nkzlfun}[1]{\ul{n}\uz_{#1}}
\newcommand{\nkzufun}[1]{\ol{n}\uz_{#1}}

\def\nzl{\ul{n}\uz}
\def\nzu{\ol{n}\uz}
\def\nnl{\ul{n}\un}
\def\nnu{\ol{n}\un}
\def\nil{\ul{n}\ui}
\def\niu{\ol{n}\ui}

\def\nkzl{\ul{n}\uz_k}
\def\nkzu{\ol{n}\uz_k}
\def\nknl{\ul{n}\un_k}
\def\nknu{\ol{n}\un_k}

\def\yknow{y_k^{(\tnow)}}
\def\nknow{n_k^{(\tnow)}}

\newcommand{\nk}{n_k}
\newcommand{\nkp}{n_k'}
\newcommand{\yk}{y_k}
\newcommand{\ykp}{y_k'}

\def\taut{\tau(\vec{t})}
\def\ttau{\tilde{\tau}}
\def\ttaut{\ttau(\vec{t})}
\def\tautk{\tau(\vec{t}_k)}

\def\MZ{\mathcal{M}\uz}
\def\MN{\mathcal{M}\un}

\def\MkZ{\mathcal{M}\uz_k}
\def\MkN{\mathcal{M}\un_k}

\def\PkZ{\Pi\uz_k}
\def\PkN{\Pi\un_k}
\newcommand{\PZi}[1]{\Pi\uz_{#1}}

\def\tnow{t_\text{now}}
\def\tpnow{t^+_\text{now}}

\newcommand{\Rsysnow}{R^{(t_\text{now})}_\text{sys}}
\newcommand{\Tsysnow}{T^{(t_\text{now})}_\text{sys}}
\newcommand{\tsysnow}{t^{(t_\text{now})}_\text{sys}}
\newcommand{\fsysnow}{f^{(t_\text{now})}_\text{sys}}
\def\eknow{e_k^{(\tnow)}}
\def\cknow{c_k^{(\tnow)}}
\def\vectknow{\vec{t}_k^{(\tnow)}}
\def\Phinow{\Phi^{(\tnow)}}
\newcommand{\gnow}{g^{(\tnow)}}
\newcommand{\tausnow}{\tau_*^{(\tnow)}}
\newcommand{\tprep}{\tau_{\text{prep}}}
\newcommand{\tthresh}{\tau_{\text{thresh}}}
\newcommand{\tstarnow}{t_*^{(\tnow)}}
\newcommand{\gstarnow}{g_*^{(\tnow)}}
\newcommand{\gtotalnow}{g_\text{total}^{(\tnow)}}
\newcommand{\esys}{e_\text{sys}}
\newcommand{\mrsys}{\bar{r}_\text{sys}}

% ------------ options -------------

\allowdisplaybreaks

\journal{RESS}

\begin{document}

% ------------ frontmatter -------------

\begin{frontmatter}
\title{Condition-Based Maintenance for Complex Systems\\ based on Current Component Status\\ and Bayesian Updating of Component Reliability}

\author[tue]{Gero Walter}
\ead{g.m.walter@tue.nl}
\author[tue]{Simme Douwe Flapper}
\ead{s.d.p.flapper@tue.nl}

\address[tue]{School of Industrial Engineering, Eindhoven University of Technology, Eindhoven, Netherlands}


\begin{abstract}
We propose a new condition-based maintenance policy for complex systems,
based on the status (working, defective) of all components within a system,
as well as the reliability block diagaram of the system.
%
By means of the survival signature,
a generalization of the system signature allowing for multiple component types,
we obtain a predictive distribution for the system survival time,
%(also known as RUL, remaining useful life)
also known as residual life distribution,
based on which of the system's components currently function or not,
and the current age of the functioning components.

The time to failure of the components of the system
is modeled by a Weibull distribution with a fixed shape parameter.
The scale parameter is iteratively updated in a Bayesian fashion
using the current (censored and non-censored) component lifetimes.
Each component type has a separate Weibull model that may also include test data.

The cost-optimal moment of replacement for the system is obtained by minimizing
the expected cost rate per unit of time.
The unit cost rate is recalculated when components fail
or at the end of every (very short) fixed inter-inspection interval,
leading to a dynamic maintenance policy,
since the aging of components and possible failures will change the cost-optimal moment of repair in the course of time.
Via numerical experiments, some insight into the performance of the policy is given.
\end{abstract}

\begin{keyword}
condition-based maintenance \sep system reliability \sep remaining useful life \sep survival signature \sep unit time cost rate\end{keyword}
\end{frontmatter}


% ------------ manuscript -------------

\section{Introduction}
\label{intro}

Both in practice and academics,
there is a growing interest in CBM (condition-based maintenance), see e.g.,
\citet[pp.~15--55]{2016:oldekeizer}
The central idea behind CBM is to maintain systems or components at the right time,
i.e., before they fail, but not too early, in order to keep their reliability high and operating costs low.
In this context a trade-off is made between the risk of failure during operation
(which can lead to costly downtime: idle workforce, missed production, penalties, loss of reputation)
and the costs of premature maintenance (wasting potential component or system lifetime,
downtime cost, cost of executing unnecessary maintenance activities).

Among the reasons for the above interest are the importance of increasingly short and
reliable delivery times, and decreasing profit margins due to worldwide competition.
All kinds of technical improvements have made it possible % it is more / better / earlier possible
to estimate the condition of systems as a whole as well as their undelying components,
whereas the cost for these technologies is decreasing rapidly,
making the use of CBM policies even more feasible.

Most CBM policies are based on a directly observable, continuously measurable condition or degradation signal.
Alternatively, such a signal or health status is constructed
using indirect measurements to determine the remaining useful life or time to failure of a system,
see, e.g., \citet{2014:rul-review, 2011:rul-review-statistical}.

In this paper, we propose a kind of inspection-based CBM policy
for when no degradation signal for the system is available,
but where the status (working or not working) of the components can be monitored continuously.
In this situation, one can use the system's reliability block diagram
and information with respect to the status of its components
to directly calculate the system residual life distribution,
and base the maintenance policy for the system on this distribution.
In fact, our CBM policy can be seen as based on a multivariate degradation signal,
where each component sends a binary signal, and the reliability block diagram is used for sensor fusion.

There are 3 types of triggers to review the system in our new policy:
at the end of each very short inspection interval (1),
directly after the failure of a component where the system still functions (2),
and directly after the failure of a component due to which the system as a whole no longer can function (3). 
We denote each review of the system in reaction to these triggers by `evaluation',
since the system RLD is re-evaluated at each of these time points.
We use the term `evaluation' in place of the term `inspection',
because the re-evaluation of the system RLD does not require a physical inspection,
as we assume to learn of the exact failure times of components online via continuous monitoring.

In case of a trigger of type (1),
the currently used model for component failure times is updated
to account for the aging of components,
and the extra lifetime observed for the components.
The latter information is taken into account via right-censored observations
in a Bayesian parameter update step.

In case of a trigger of type (2),
when a component has failed since the last inspection but the system is still functioning,
also the reliability block diagram of the system is updated.
Based on the above changes,
it is determined in both cases whether the system should be replaced now,
or that this decision should be postponed until the next planned evaluation moment,
accepting the risk of a system breakdown before.
In this context, the optimal moment of maintenance
is calculated based on minimizing the expected maintenance-related cost per unit of time for the present operational cycle,
where an operational cycle starts with a new system, and ends after this system has been replaced by an as good as new system. 

In case of a trigger of type (3),
the currently used component failure time distributions are updated
like in case of a trigger of type (1) and (2)
to account for the information gained on components
until the moment of system failure.
Hereafter, a new system is installed,
using the updated models for all components based on the insight obtained from the concluded operational cycle.

At first sight, the above quasi-continous updating of the component failure models,
and the system reliability block diagram,
as well as redetermining the best moment to replace the system,
seems to result in a (very) nervous maintenance policy.
However, due to the very short calculation times,
only at certain moments in time people involved in the execution of maintenance tasks
need to receive detailed information,
whereas during the remaining time,
all can be handled automatically, as done at the moment within, e.g., many safety systems. 

The setup of the rest of the paper is as follows.
In Section~\ref{sec:literature}, we describe our contribution to the CBM literature.
Next, in Sections~\ref{sec:sysrel}, \ref{sec:adaptive-sysrel-weibull} and \ref{sec:policy},
the three main steps in our policy are discussed in detail.
Hereafter, in Sections~\ref{sec:examples} and \ref{sec:sim},
our policy is applied further to an example.
Finally, in Section~\ref{sec:outlook},
a short summary and our main conclusions are given,
followed by some suggestions for further research.

\iffalse
% ----------------------------------------------------------------------------------------------------
\subsection*{---------------------}
***general discussion on CBM as usual

***Condition-based maintenance (CBM) has received considerable attention in the literature.
The central idea is to maintain technical systems or components at just the right time,
that is, before they fail,
but not too early, in order to keep reliability high and operating costs low.
%most fully use the component's or system's lifetime and to save on maintenance work

***trade-off between risk of failure during operation
(can lead to costly downtime: idle workforce, missed production, penalties, loss of reputation)
and costs of premature maintenance
(wasting potential component / system lifetime, downtime cost, cost of maintenance work)

***so CBM must be based on some information about the state or health of the component / system.
Two ways: continuous monitoring CBM and inspection-based CBM.

***Continuous monitoring CBM policies are usually derived
using a directly observable continuously measurable condition / degradation signal,
or constructs such a signal or health status using indirect measurements.
Estimated time till failure (RUL) \citep{2014:rul-review, 2011:rul-review-statistical}
via distance of current signal level to a fixed known failure threshold.

***Inspection-based CBM via delay time model, modeling the time between detectable degradation and failure,
again time till failure via distance of current degradation level to fixed known failure threshold.

***In both cases, maintenance decision via control limit / threshold for the signal
(RUL not explicitely calculated)
by minimizing the expected unit cost rate,
which is often approximated using the renewal reward theorem.
%but in practice threshold often not known
\subsection*{---------------------}

Here we propose a different approach to CBM
for the case when no degradation signal for the system is available,
but system components can be identified and their functioning status
(working or not working) can be monitored.
In this situation, one can use the system's layout in reliability terms
(i.e., its reliability block diagram) and information on components' status
to directly calculate the residual life distribution (RLD),
and base the maintenance policy on this distribution.
(In fact, our approach can be seen as CBM based on a multivariate degradation signal,
where each component sends a binary signal, and the reliability block diagram is used
for sensor fusion.)

Our approach can also be seen as a generalization of policies for $k$ out of $N$ systems to arbitrary system layouts.
Furthermore, our approach allows for multiple types of components in the system,
with each component type having its own failure behaviour.

%We use a parametric model not for a degradation signal of the system or for the delay time, but for component lifetimes:
For each component type, we use the well-known Weibull distribution to model component lifetimes.
To keep the model simple and to demonstrate its feasability, we assume the shape parameter for each component type to be known,
focusing on learning of the scale parameter,
and that components fail independently.
These simplifying assumptions will often not hold in practice,
then the model can serve as a first approximation.
Generally, we see our contribution as opening up an entirely new way to derive CBM policies,
and thus consider the model in this paper more as a proof of concept,
where extension to more realistic models must happen in a second step.
As an example, it is possible to extend the model to learn also the shape parameter along the lines of
\cite{1969:soland},
but this is not included here as it would complicate presentation.
%need only to observe when a component fails,
%and expert information about expected component failure times.

The Bayesian approach to the Weibull model \citep[see, e.g.,][]{1996:mazzuchi-soyer} allows to integrate, for each component type,
three sources of information: expert knowledge, data from component tests,
and the status of components in the monitored system. %, i.e., current system condition.
These information sources are integrated into a single component model
using the iterative nature of Bayesian updating,
leading to a so-called posterior predictive distribution for the number of components surviving at any time in the future.

These predictive distributions are then used to calculate the exact distribution of system residual life
using the survival signature \citep{2012:survsign}.
(***we therefore accomplish the same as \cite{2013:si-et-al} for their situation.)

By its Bayesian nature, the system residual life distribution adequately reflects the uncertainties %in RUL estimation
related to modeling and prediction \citep{2015:sankararaman},
readily adapting to any changes in component behaviour.
The use of conjugate priors means that we get explicit formulas for the RLD,
so no numerical integration or simulation techniques are necessary.
This allows for a very frequent, or even real-time, update of the RLD,
taking into account the changed structure when components have failed
and the information gain from updating the component reliability distributions.

Based on the RLD at any current time $\tnow$,
the optimal moment of maintenance given all information available at $\tnow$
is then determined by minimizing the expected one-cycle unit cost rate.
The expected one-cycle unit cost rate, also known as one-cycle criterion
\citep{1984:ansell-bendell-humble,1996:mazzuchi-soyer,2006:coolen-schrijner-coolen},
makes the trade-off between preventive and corrective maintenance according to the current cycle only,
unlike the usually employed renewal based criterion,
which approximates the unit cost rate using a renewal argument \citep[p.~296]{1996:mazzuchi-soyer},
but is not suited for situations where one may whish to change the strategy per cycle \citep{2006:coolen-schrijner-coolen}.

Since the RLD changes with current time $\tnow$,
the corresponding optimal moment of maintenance $\tausnow$ changes as well,
leading to a dynamic adaptive maintenance policy,
similar to a CBM policy based on a continuosly monitored degradation signal.
However, unlike such threshold-based CBM policies,
our approach allows to easily take into account the time needed to set up maintenance work (known as set-up time),
since it gives the optimal moment of maintenance as a time beyond current time $\tnow$,
and maintenance can be initiated as soon as $\tausnow$ equals the set-up time.

In this paper, we assume that at the moment of maintenance,
all components in the system are replaced,
so the cost parameters in the unit cost rate must be determined accordingly.
One could instead consider other replacement schemes (e.g., replacing only the failed components),
or indeed optimize the unit cost rate over all possible replacement schemes,
since our method for RLD calculation can handle differently aged components. 
However, optimizing over replacement schemes opens up a full research programme on its own,
and for situations with high set-up costs,
as is typical for CBM applications,
the replace-all scheme is most likely the cost-optimal strategy. 

***figure of process: component priors $\to$ component posteriors $\to$ system RLD $\to$ $\gnow(\tau)$ $\to$ $\tausnow$

***operation: recalculate $\tausnow$ at fixed dense grid of time points (quasi-continuous),
this makes sense even when no failures happen because the fact that components in the system still function
gives information on component distributions;
alternatively, do the recalculation every time when a component fails (failures contain the most information)

***model allows also to switch to corrective policy when $\gnow(\tau)$ monotonely decreasing (then $\tausnow \to \infty$)

***Expert input in our model is, for each component type,
Weibull shape parameter, expected failure time (translated to scale parameter),
and expert info weight (how sure about mean failure time guess).
Component test data (can include right-censored observations) is optional.
Further input is system layout (reliability block diagram indicating which component belongs to which type)
and the cost parameters $c_p$ and $c_u$.

***here all expressed in time, but could also be in usage (number of cycles, etc).

***compare to $M^*$ out of $N$ policy:
we are dynamic (would mean for a $M^*$ out of $N$ policy that $M^*$ decreases over time due to component aging),
we learn about component lifetimes,
and we allow for set-up time
\fi
% ----------------------------------------------------------------------------------------------------

\section{Literature review}
\label{sec:literature}

Although there is a lot of literature on CBM policies for multi-component systems,
see, e.g., \citet{2016:oldekeizer},
we found only a few papers having some relation to our approach.

The paper that comes closest to this paper is the paper by \citet{2013:si-et-al}.
In that paper, the condition of a system is inspected,
where the moment of the next inspection is determined via an updated degradation path based on the present condition of the system.
The authors provide exact expressions for the RUL (remaining useful lifetime) distribution,
also known as residual life distribution (RLD),
which is updated in an empirical Bayesian framework using conjugate priors.
The RLD is used to construct a replacement decision model using a cost rate.

One of the main differences between \citet{2013:si-et-al} and our paper
is that we use component status data and the system reliability block diagram instead of the degradation signal
as basis for calculating the system RUL.
Another main difference is that our policy can be applied to arbitrary system configurations
(i.e., arbitrary reliability block diagrams with arbitrary components),
whereas the method given by \citet{2013:si-et-al} can only be applied to single components.
A third difference concerns how the cost rate is calculated.
\citet{2013:si-et-al} calculate this rate using the renewal-reward theory approximation.
Essential for using the renewal-reward theory for calculating the expected cost per unit of time
is that the relevant characteristics of the system considered and its environment are not changing,
i.e. that the failure behavior of the system,
in essence its relibability function, does not change over time,
and that we use the same policy with the same parameter values during a statistically long enough period of time.
However, due to the update method related to the CBM policy presented in this paper
like in \citet{2013:si-et-al}, we cannot use this approximate calculation method,
and therefore use the exact formula for the average cost per unit of time.

Another paper that includes some of the aspects included in our paper is the paper by \citet{2012:sun-et-al}.
The authors construct a health index for a system based on sensor measurements,
where the health status prediction is updated sequentially,
resulting in a RUL distribution like with our method.
However, in contrast to our paper, the authors do not link their RUL distribution to any maintenance decisions.

\citet{2011:kim-et-al} developed a periodic monitoring CBM policy
where a maintenance decision is triggered when a Bayesian control chart
(a sequentially updated health indicator) exceeds a control limit (threshold)
that is determined by minimizing the expected average cost per time unit.
We use the same cost criterion, but base the maintenance policy decision directly on our exact RLD.

To summarize the main contribution of our paper,
it introduces a CBM-like policy that does not require knowledge of the degradation path of the system,
but can take into account continuous changes in the system reliability block diagram,
as well as changes in the component failure time distributions.



% ----------------------------------------------------------------------------------------------------
\iffalse
***CBM via RUL/RLD, $M^*$ out of $N$ policies?
***also generally Bayesian methods in maintenance optimization?
***cite CBM review Olde Keizer, Flapper, Teunter, and other refs from it?

Example for Bayesian method in CBM via inspections: \cite{2007:wang-jia}
propose a model that uses Bayesian updating,
but the model aims to identify an optimal inspection interval.
%This is CBM via inspections, not CBM via continuous monitoring as we aim to do.

\begin{scriptsize}
Wang \& Jia (2007) have a model for defects arrival (homogeneous Poisson process with rate $\lambda$, so no aging),
model for delay time (time between defect and failure) is Weibull $h \sim \wei(\alpha,\beta)$,
and each of $\lambda, \alpha, \beta$ has a Gamma prior distribution.
The expected overall cost rate is then used to find a cost-optimal inspection interval.
For this they assume that failures are immediately repaired,
and defects found in an inspection are repaired during inspection.

They use prior predictive distributions for certain statistics (number of defects, number of failures, \ldots between $0$ and $T$)
to obtain prior values for the Gamma hyperparameters,
by solving prior predictive equations -- but give no detail on how they actually do this.

In the end they actually do not advocate to calculate the posterior expected cost rate (15),
but rather plug in posterior estimates for $\lambda, \alpha, \beta$,
so neglecting uncertainty in estimation.
(We do not assume a parametric distribution for delay time,
it rather arises as a consequence of the system layout and component failure distributions.)

\end{scriptsize}

Example for Bayesian method in CBM with continuous monitoring: \cite{2011:elwany-et-al}
(exponential degradation model, whose parameters $\theta'$ and $\beta$ each have a prior
which is updated using the degradation signal history,
and use total expected infinite-horizon discounted cost to determine the maintenance policy,
RUL not explicitely calculated)

Example for Bayesian method in prediction of system remaining useful life: \cite{2012:sun-et-al}
(constructs health index for a system based on sensor measurements,
health status prediction is updated sequentially,
leads to RUL distribution like our method,
but no link from RUL to maintenance decision)

\cite{2013:si-et-al} do CBM with continuous monitoring for a single component,
taking into account the whole degradation path history.
The authors provide exact expressions for the RUL distribution, which is updated in an empirical Bayesian framework using conjugate priors.
The RUL distribution is used to construct a replacement decision model using the unit cost rate via renewal reward
(eq. 35)\\
***this is most similar to what we do: exact RUL distribution $\to$ maintenance decision,
but we have component status instead of degradation signal as basis for RUL,
and we model epistemic uncertainties!\\

\cite{2011:kim-et-al} develop a periodic monitoring CBM policy
where a maintenance decision is triggered when
a Bayesian control chart (a sequentially updated health indicator) %posterior probability that system is in a 'warning' state
exceeds a control limit (threshold) that is determined
by minimizing the expected average cost per time unit.
(We use the same cost criterion, but base the maintenance decision directly on our exact RUL estimation.)

***typical recent example for Bayesian network models in maintenance -- could also cover system reliabilty terrain***
maybe Jones et al (RESS 95:3, 2010) ``The use of Bayesian network modelling for maintenance planning in a manufacturing industry'',
or Bouaziz et al (2013) ``Towards Bayesian network methodology for predicting equipment health factor of complex semiconductor systems''

\begin{scriptsize}
Bayesian networks (BNs) are a very general method to jointly model multiple dependent random variables in a Bayesian way. 
This needs assessments of conditional independece relations, and conditional probability models for each variable,
the latter often in form of conditional probability tables (discrete variables).
A Bayesian network models probabilistic relations between variables,
unlike a reliability block diagram, which gives a deterministic relation
between the status of components and the system state.

One could model a system as a BN (each component is a variable),
but that would add a lot of complications.
Most algorithms for BNs assume discrete distributions,
and do not scale well to large networks (many tasks are NP-hard).

The above papers seem to use a BN to model a degradation signal or (system) health index (have to check again to be sure).
Our approach is different, as we get an exact formulation for the system RUL directly.
\end{scriptsize}
\fi
% ----------------------------------------------------------------------------------------------------


\section{Reliability function for complex systems using the survival signature}
\label{sec:sysrel}

Here we describe how the system reliability function, given the system reliability block diagram and arbitrary component models,
can be efficiently calculated using the survival signature.
%We will show in Section~\ref{sec:adaptive-sysrel-weibull} how, for a certain choice of component model,
%this method can be used to find the system reliability function $\Rsysnow(t)$ at current time $\tnow$ for a monitored system,
%giving us the residual life distribution on which we base our adaptive maintenance policy.

We can analyze systems with an arbitrary reliability block diagram,
consisting of components of $K$ different types,
where there are $N_k$ exchangeable components of type $k$ in the system,
and the total number of components in the system is $\sum_{k=1}^K N_k = N$.

As a running example, we consider a simplified automotive braking system
consisting of four component types $M$, $H$, $C$ and $P$,
with reliability block diagram as depicted in Figure~\ref{fig:brakesys-layout}.
The master brake cylinder ($M$) activates all four wheel brake cylinders ($C_1$ -- $C_4$),
which in turn actuate a braking pad assembly each ($P_1$ -- $P_4$).
The hand brake mechanism ($H$) directly actuates the brake pad assemblies $P_3$ and $P_4$;
the vehicle brakes when at least one brake pad assembly is actuated.
Note that because of the `handbrake shortcut', this system cannot be described as a nesting of series and parallel subsystems.
%This system is observed until time $\tnow$,
%leading to censored observation of certain component lifetimes within the system.

\begin{figure}
\centering
\begin{tikzpicture}
[typeM/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=8mm}, %font=\footnotesize},
 typeC/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=8mm}, %font=\footnotesize},
 typeP/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=8mm}, %font=\footnotesize},
 typeH/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=8mm}, %font=\footnotesize},
 type1/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 type2/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 type3/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 cross/.style={cross out,draw=red,very thick,minimum width=9mm, minimum height=7mm},
 hv path/.style={thick, to path={-| (\tikztotarget)}},
 vh path/.style={thick, to path={|- (\tikztotarget)}}]
\begin{scope}[xscale=1.5, yscale=1.2]
\node[typeM] (M)    at ( 0  , 0  ) {$M$};
\node[typeC] (C1)   at ( 1  , 1.5) {$C_1$};
\node[typeC] (C2)   at ( 1  , 0.5) {$C_2$};
%\node[cross]        at ( 1  , 0.5) {};
\node[typeC] (C3)   at ( 1  ,-0.5) {$C_3$};
%\node[cross]        at ( 1  ,-0.5) {};
\node[typeC] (C4)   at ( 1  ,-1.5) {$C_4$};
\node[typeP] (P1)   at ( 2  , 1.5) {$P_1$};
\node[typeP] (P2)   at ( 2  , 0.5) {$P_2$};
%\node[cross]        at ( 2  , 0.5) {};
\node[typeP] (P3)   at ( 2  ,-0.5) {$P_3$};
%\node[cross]        at ( 2  ,-0.5) {};
\node[typeP] (P4)   at ( 2  ,-1.5) {$P_4$};
\node[typeH] (H)    at ( 0  ,-1  ) {$H$};
\coordinate (start)  at (-0.7, 0);
\coordinate (startC) at ( 0.5, 0);
\coordinate (startH) at (-0.4, 0);
\coordinate (Hhop1)  at ( 0.4,-1);
\coordinate (Hhop2)  at ( 0.6,-1);
\coordinate (endP)   at ( 2.5, 0);
\coordinate (end)    at ( 2.8, 0);
\path (start)     edge[hv path] (M.west)
      (M.east)    edge[hv path] (startC)
      (startC)    edge[vh path] (C1.west)
                  edge[vh path] (C2.west)
                  edge[vh path] (C3.west)
                  edge[vh path] (C4.west)
      (C1.east)   edge[hv path] (P1.west)
      (C2.east)   edge[hv path] (P2.west)
      (C3.east)   edge[hv path] (P3.west)
      (C4.east)   edge[hv path] (P4.west)
      (endP)      edge[vh path] (P1.east)
                  edge[vh path] (P2.east)
                  edge[vh path] (P3.east)
                  edge[vh path] (P4.east)
                  edge[hv path] (end)
      (startH)    edge[vh path] (H.west)
      (H.east)    edge[hv path] (Hhop1)
      (Hhop1)     edge[thick,out=90,in=90] (Hhop2)
      (Hhop2)     edge[hv path] (P3.south)
                  edge[hv path] (P4.north);
\end{scope}
\end{tikzpicture}
\caption{Reliability block diagram for a simplified automotive brake system
with four component types $M$, $H$, $C$ and $P$.}
%Note that this system layout cannot be expressed as a nesting of series and parallel layouts.
%The corresponding survival signature $\Phi(l_M,l_H,l_C,l_P)$ is given in Table~\ref{tab:brakesys-survsign}.}
\label{fig:brakesys-layout}
\end{figure}

In a system with $N$ components, the state of the system can be expressed by the state vector
$\vec{x} = (x_1,x_2,\ldots,x_N) \in \{0,1\}^N$,
with $x_i=1$ if the $i$th component functions and $x_i=0$ if not.
The structure function $\phi : \{0,1\}^N \rightarrow \{0,1\}$, defined for all possible $\vec{x}$,
takes the value 1 if the system functions and 0 if the system does not function for state vector $\vec{x}$ \citep{BP75}.
Most real-life systems are coherent,
which means that $\phi(\vec{x})$ is non-decreasing in any of the components of $\vec{x}$,
so system functioning cannot be improved by worse performance of one or more of its components.
Furthermore, one can usually assume that $\phi(0, \ldots, 0) = 0$ and $\phi(1, \ldots, 1) = 1$.

The survival signature \citep{2012:survsign} is a summary of the structure function
for systems with $K$ groups of exchangeable components.
Denoted by $\Phi(l_1,\ldots,l_K)$, with $l_k=0,1,\ldots,N_k$ for $k=1,\ldots,K$,
it is defined as the probability for the event that the system functions
given that precisely $l_k$ of its $N_k$ components of type $k$ function, for each $k\in \{1,\ldots,K\}$.
Essentially, this creates a $K$-dimensional partition for the event $\Tsys > t$,
such that $\Rsys(t) = P(\Tsys > t)$ can be calculated using the law of total probability,
\begin{align}
P(\Tsys > t) &= \sum_{l_1=0}^{N_1} \cdots \sum_{l_K=0}^{N_K} P(\Tsys > t \mid C^1_t = l_1,\ldots, C^K_t = l_K)
                                                                                  P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \nonumber\\
             &= \sum_{l_1=0}^{N_1} \cdots \sum_{l_K=0}^{N_K} \Phi(l_1,\ldots,l_K) P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \nonumber\\
             &= \sum_{l_1=0}^{N_1} \cdots \sum_{l_K=0}^{N_K} \Phi(l_1,\ldots,l_K) \prod_{k=1}^K P(C^k_t = l_k)\,,
\label{eq:sysrel-survsign}
\end{align}
where $P(C^k_t = l_k)$ is the (predictive) probability that exactly $l_k$ components of type $k$ function at time $t$.
The last equality holds as we assume that components fail independently.
Note that for coherent systems, the survival signature $\Phi(l_1,\ldots,l_K)$ is non-decreasing in each $l_k$.

Continuing our example,
the survival signature for the system in Figure~\ref{fig:brakesys-layout} is given in Table~\ref{tab:brakesys-survsign},
omitting the entries for which $\Phi(l_M, l_H, l_C, l_P) = 0$ or $\Phi(l_M, l_H, l_C, l_P) = 1$,
because the full table would contain $\prod_{k=1}^K (N_k + 1) = 2 \times 2 \times 5 \times 5 = 100$ rows.

\begin{table}
\centering
\begin{tabular}{cccclcccccl}
  \toprule
$l_M$ & $l_H$ & $l_C$ & $l_P$ & $\Phi$ & \quad & $l_M$ & $l_H$ & $l_C$ & $l_P$ & $\Phi$\\ 
  \midrule
1 & 0 & 1 & 1 & 0.25 & & 1 & 0 & 2 & 1 & 0.50 \\ 
1 & 0 & 1 & 2 & 0.50 & & 1 & 0 & 2 & 2 & 0.83 \\ 
1 & 0 & 1 & 3 & 0.75 & & 1 & 0 & 3 & 1 & 0.75 \\ 
0 & 1 & 0 & 1 & 0.50 & & 1 & 1 & 0 & 1 & 0.50 \\ 
0 & 1 & 0 & 2 & 0.83 & & 1 & 1 & 0 & 2 & 0.83 \\ 
0 & 1 & 1 & 1 & 0.62 & & 1 & 1 & 1 & 1 & 0.62 \\ 
0 & 1 & 1 & 2 & 0.92 & & 1 & 1 & 1 & 2 & 0.92 \\ 
0 & 1 & 2 & 1 & 0.75 & & 1 & 1 & 2 & 1 & 0.75 \\ 
0 & 1 & 2 & 2 & 0.97 & & 1 & 1 & 2 & 2 & 0.97 \\ 
0 & 1 & 3 & 1 & 0.88 & & 1 & 1 & 3 & 1 & 0.88 \\ 
   \bottomrule
\end{tabular}
\caption{Survival signature $\Phi(l_M, l_H, l_C, l_P)$
for the simplified automotive brake system depicted in Figure~\ref{fig:brakesys-layout},
omitting the rows for which $\Phi(l_M, l_H, l_C, l_P) = 0$ or $\Phi(l_M, l_H, l_C, l_P) = 1$.}
\label{tab:brakesys-survsign}
\end{table}


\section{Adaptive system residual life distribution based on Weibull component models}
\label{sec:adaptive-sysrel-weibull}

In this section, we describe how the system reliability function $\Rsysnow(t)$ at current time $\tnow$ 
can be calculated for a specific choice of component model.
%Section~\ref{sec:policy} will then show how an adaptive maintenance policy
%can be derived for a monitored system based on such a current system residual life distribution. 

We consider the well-known Weibull model for component lifetimes,
which is used in a wide variety of reliability studies. 
To keep things simple, we assume that the shape parameter of the Weibull distribution is known,
and that only the scale parameter needs to be estimated.
The model could be extended to learn also the shape parameter in a later step,
using, e.g., the discretized approach by \cite{1969:soland}.

By using a Bayesian approach, described in Section~\ref{sec:weibull},
our component model allows to include both expert assessments and test data (if available),
and furthermore can account for component lifetime information from the current operational cycle up to $\tnow$.
%***component model: weibull with fixed shape,
%inverse gamma prior on scale parameter,
%fix priors via expected lifetime and prior strength,
%test data inclusion
%with noninformatiove right-censoring
%
In Section~\ref{sec:postpred} we derive the posterior predictive distribution
for the Weibull component model as needed for the current system reliability calculation.
Finally, in Section~\ref{sec:sysreltnow}, we adapt the method from Section~\ref{sec:sysrel} for the adaptive setting,
resulting in a formula for $\Rsysnow(t)$.
%***output is $\Rsysnow(t)$, the current (at time $\tnow$) system reliability function
%(residual life distribution, RLD)
%taking into account the current system state,
%including the current ages of system components, 
%and the lifetime histories of all component types,
%including test data (if available) and expert assessments.

The model description below follows closely \cite{2016:walter-coolen},
who presented the same residual life distribution model.
However, \cite{2016:walter-coolen} did not consider maintenance policies,
but focus instead on a generalization of the RLD model using sets of priors for the component models.
%These sets of priors can be seen as parametric P-boxes,
%a certain kind of imprecise probability model \citep[see, e.g.,][]{itip}.
%Such models allow for vague and partial prior specifications,
%and provide sensitivity to prior-data conflict \cite[\S 2.2.3.3]{diss}.


\subsection{Bayesian update of the Weibull component models}
\label{sec:weibull}

Here we describe the Weibull model for the component lifetimes,
together with the Bayesian update procedure which allows to include
expert knowledge, component tests, and information from the monitored components in the system.

For each type $k$ component, we assume that the lifetime $T_{k,i}$ ($i=1,\ldots,N_k$, $k = 1, \ldots, K$)
is Weibull distributed with scale parameter $\lambda_k$ and (fixed) shape parameter $\beta_k > 0$,
in short $T_{k,i} \mid \lambda_k \sim \wei(\beta_k,\lambda_k)$,
with pdf (probability density function) and cdf (cumulative distribution function)
\begin{align}
\label{eq:weibulldens}
f(t_{k,i} \mid \lambda_k) &= \frac{\beta_k}{\lambda_k} (t_{k,i})^{\beta_k-1} e^{-\frac{(t_{k,i})^{\beta_k}}{\lambda_k}}\,, \\
\label{eq:weibullcdf}
F(t_{k,i} \mid \lambda_k) &= 1 - e^{-\frac{(t_{k,i})^{\beta_k}}{\lambda_k}} = P(T_{k,i} \leq t_{k,i} \mid \lambda_k)\,,
\end{align}
where $\lambda_k > 0$ and $t > 0$.

The shape parameter $\beta_k$ determines whether the hazard rate is increasing ($\beta_k > 1$)
or decreasing ($\beta_k < 1$) over time.
For $\beta_k=1$, one obtains the Exponential distribution with constant hazard rate as a special case.
The scale parameter $\lambda_k$ can be interpreted through the relation
\begin{align}
\E[T_{k,i} \mid \lambda_k] &= \lambda_k^{1/\beta_k}\, \Gamma(1 + 1/\beta_k)\,.
\label{eq:lambdainterpret}
\end{align}
We will use this equation to convert expected lifetimes to $\lambda_k$ and vice versa.

With a Bayesian approach, one can express expert knowledge about the reliability of the components
by assigning a so-called prior distribution,
a distribution over the unknown parameter $\lambda_k$.
This prior distribution $f(\lambda_k)$ is then updated 
to the so-called posterior distribution $f(\lambda_k \mid \vec{t})$,
the distribution over $\lambda_k$ given the data $\vec{t}$,
via Bayes' rule
\begin{align*}
f(\lambda_k \mid \vec{t}) &\propto f(\vec{t}\mid\lambda_k) f(\lambda_k)\,.
\end{align*}
The posterior $f(\lambda_k \mid \vec{t})$ subsumes the information
from both expert knowledge and data,
and forms the basis for all inferences, like, e.g., predictions.

For the prior over $\lambda_k$,
a convenient choice is to use the inverse Gamma distribution,
which is commonly parametrized in terms of the parameters $a_k > 0$ and $b_k > 0$,
\begin{align}
f(\lambda_k\mid a_k,b_k) &= \frac{(b_k)^{a_k}}{\Gamma(a_k)} \lambda_k^{-a_k -1} e^{-\frac{b_k}{\lambda_k}}\,,
\label{eq:ig-def}
\end{align}
in short, $\lambda_k \mid a_k, b_k \sim \ig(a_k,b_k)$.
%Here, we have added the prior parameters $a_k$ and $b_k$ in notation
%to indicate that the prior over $\lambda$ depends on their values.

The inverse Gamma distribution is convenient because it is a conjugate prior,
i.e., the posterior obtained by Bayes' rule is again an inverse Gamma distribution and thus easily tractable
because only the prior parameters need to be updated to obtain the posterior parameters,
so no numerical integation or simulation techniques are necessary.
Furthermore, this conjugacy holds also when right-censored observations are used for updating,
as indicated below.

Instead of the usual parametrization in terms of $a_k$ and $b_k$,
we use the parameters $\nk > 1$ and $\yk > 0$
%(the upper index ${}\uz$ is used to indicate that these are prior parameters)
because they are more easy to interpret.
They are defined as
\begin{align}
\nk &= a_k - 1 & &\text{ and}
&
\yk &= b_k / \nk,
\label{eq:abtony}
\end{align}
where $\yk$ can be interpreted as the prior guess for the scale parameter $\lambda_k$,
as $\E[\lambda_k\mid\nk,\yk] = \yk$.
Using \eqref{eq:lambdainterpret},
we can thus translate an expert's statement of the expected component lifetime into a corresponding value for $\yk$.
$\nk$ can be seen as a prior strength or pseudocount,
this will become clear in the discussion of the update step below.

The parametrization in terms of $\nk$ and $\yk$ also clarifies the nature of the combination
of prior information and data through Bayes' rule.
In the conjugate setting,
applying Bayes' rule simply means that the prior parameters $\nk$ and $\yk$
are updated to posterior parameters, which we denote by $\nkp$ and $\ykp$, respectively.
Assume we observe $N_k = e_k + c_k$ component lifetimes,
where $e_k$ is the number of actual failure events,
and $c_k$ is the number of right-censored observations.
We denote the failure times by $t_{k,1}, \ldots, t_{k,e_k}$,
and the censoring times by $t^+_{k,1}, \ldots, t^+_{k,c_k}$,
and collect them in an observation vector $\vec{t}_k = (t_{k,1}, \ldots, t_{k,e_k}, t^+_{k,1}, \ldots, t^+_{k,c_k})$.
Then, the updated, posterior parameters are
\begin{align}
\nkp &= \nk + e_k\,, 
&
%\ykp &=  \frac{\nk \yk + \tautk}{\nk + e_k}\,,
\ykp &=  \frac{\nk}{\nk + e_k}\, \yk + \frac{e_k}{\nk + e_k}\, \frac{\tautk}{e_k}\,,
\label{eq:ig-update}
\end{align}
where $\tautk = \sum_{i=1}^{e_k} (t_{k,i})^\beta + \sum_{i=1}^{c_k} (t^+_{k,i})^\beta$.
%The upper index ${}\un$ indicates that these are posterior parameters resulting from an update with $n_k$ observations,
%where we leave out the index $k$ for $n_k$ in the upper index to increase legibility.

From the simple update rule \eqref{eq:ig-update}, we see that
$\ykp$ is a weighted average of the prior parameter $\yk$ and the maximum likelihood estimator (ML) $\tautk/e_k$,
with weights proportional to $\nk$ and $e_k$, respectively.
$\nk$ can thus be interpreted as a prior strength or pseudocount,
indicating how much our prior guess should weigh against the $e_k$ observed failure events.
Furthermore, $\V[\lambda\mid\nk,\yk] = (\yk)^2 / (1 - 1/\nk)$;
for fixed $\yk$, a higher $\nk$ indicates 
that more probability mass is concentrated around $\yk$.

Using \eqref{eq:abtony} and \eqref{eq:ig-update}, the posterior distribution over $\lambda_k$ is given by
\begin{align}
\lambda_k \mid \nk, \yk, \vec{t}_k \sim \ig(\nk + e_k + 1, \nk \yk + \tautk)\,.
\label{eq:ig-update-alpha}
\end{align}
%where we have added the prior parameters $\nkz$ and $\ykz$ in notation
%to emphasize that the posterior is a synthesis of prior information and data.
As this posterior can be defined in terms of
the updated, posterior parameters $\nkp$ and $\ykp$,
we may also write
\begin{align*}
f(\lambda_k \mid \nk, \yk, \vec{t}_k) &= f(\lambda_k \mid \nkp, \ykp)\,.
\end{align*}
The iterative nature of Bayesian inference means that we can take the updated,
posterior parameters as new prior parameters and update them again using new data.
We will use this property to use the extra information about components
that accumulates during operation of the system:
At any time $\tnow > 0$,
all non-failed components contribute a right censored observation $\tpnow$;
any failed components instead contribute a fully observed, non-censored lifetime,
and both types of observations can be used in the update step \eqref{eq:ig-update}.
%Before we write this out formally in Section~\ref{sec:postpred},
%we will first present how system reliability functions can be derived based on component models.

%To keep notation simple,
%we will below denote the parameters of the inverse Gamma distribution by $\nkz$ and $\ykz$,
%regardless of them expressing expert information alone,
%or stemming from the combination of expert information and test data.


\subsection{Component posterior predictive distributions at $\tnow$}
\label{sec:postpred}

To calculate the current system reliability function $\Rsysnow(t)$,
we need, for each $k=1,\ldots, K$, the probabilities $P(C^k_t = l_k)$
for the number of type $k$ components that function at times $t > \tnow$,
taking into account all information available at $\tnow$.
In the Bayesian framework, these probabilities are given
by a so-called posterior predictive distribution that can be derived from the posterior over $\lambda$.

Denote by $\nkz$ and $\ykz$ the parameters reflecting the knowledge base at system start-up time $t=0$.
These could have been obtained by updating prior parameters $\nk$ and $\yk$ (reflecting expert knowledge)
to $\nkp$ and $\ykp$ using test data,
or could be taken directly equal to $\nk$ and $\yk$ if no test data is available.
In both cases, the distribution $f(\lambda_k \mid \nkz, \ykz)$
thus accounts for all knowledge on component type $k$ that is available at system start-up.

Following our comments at the end of Section~\ref{sec:weibull},
the component models can be further updated
using information gained from the current run of the system until $\tnow$:
Any failure times of failed components,
and the right-censoring time $\tpnow$ for each non-failed component,
can be used to update $\nkz$ and $\ykz$ according to \eqref{eq:ig-update}.
We denote the resulting parameter values by $\nknow$ and $\yknow$.
%(It is of course possible to update $\nkz$ and $\ykz$ directly to $\nknow$ and $\yknow$ if no test data is available.)
%Furthermore, the fact that the components in the system that still function have reached the age $\tnow$
%must be used when calculating the posterior predictive distribution.

In analogue to the notation used in Section~\ref{sec:weibull},
let $N_k = \eknow + \cknow$ be the number of type $k$ components in the monitored system,
where $\eknow$ is the number of components that have failed by $\tnow$,
and $\cknow$ is the number of components that have not failed by $\tnow$.
We can thus collect these observations, as of $\tnow$, in a vector
$\vectknow = (t_{k,1}, \ldots, t_{k,\eknow}, \tpnow, \ldots, \tpnow)$,
containing $\cknow$ right-censored observations $\tpnow$.
%
The resulting posterior predictive distribution for any $t > \tnow$ is obtained as
\begin{align}
\lefteqn{%
P(C^k_t = l_k\mid\nkz,\ykz, \vectknow) }\hspace*{5.75ex} \nonumber\\  %
 &= { \cknow \choose l_k} \int \big[P(T^k >    t \mid T^k > \tnow, \lambda_k)\big]^{l_k} \times \nonumber\\ & \hspace*{3.5ex}
                               \big[P(T^k \leq t \mid T^k > \tnow, \lambda_k)\big]^{\cknow - l_k}
    f(\lambda_k\mid\nkz,\ykz,\vectknow) \dd \lambda_k\,,
\label{eq:postpredtnow}
\end{align}
where $T^k$ is the Weibull distributed lifetime of a component of type $k$.
Note that through the condition $T^k > \tnow$, we also take into account
that the components in the system have the age $\tnow$.
Now, by the Weibull assumption \eqref{eq:weibullcdf}, one has
\begin{align}
P(T^k \leq t \mid T^k > \tnow, \lambda_k)
 &= \frac{P(\tnow < T^k \leq t \mid\lambda_k)}{P(T^k > \tnow \mid \lambda_k)} \nonumber\\
 &= \frac{F(t\mid\lambda_k) - F(\tnow\mid\lambda_k)}{1-F(\tnow\mid\lambda_k)} 
% = \frac{e^{-\frac{(\tnow)^{\beta_k}}{\lambda_k}} - e^{-\frac{t^{\beta_k}}{\lambda_k}}}{e^{-\frac{(\tnow)^{\beta_k}}{\lambda_k}}}
  = 1 - e^{-\frac{t^{\beta_k} - (\tnow)^{\beta_k}}{\lambda_k}}\,.
\label{eq:weibullcondprob}
\end{align}
Consider the posterior \eqref{eq:ig-update-alpha} obtained using observations $\vectknow$,
written in terms of the updated parameters $\nknow$ and $\yknow$.
Substituting this posterior and \eqref{eq:weibullcondprob} into \eqref{eq:postpredtnow}, we get
\begin{align}
\lefteqn{P(C^k_t = l_k\mid\nkz,\ykz, \vectknow)}\hspace*{5.75ex} \nonumber\\
 &= { \cknow \choose l_k} \int \Big[    e^{-\frac{t^{\beta_k} - (\tnow)^{\beta_k}}{\lambda_k}}\Big]^{l_k}
                               \Big[1 - e^{-\frac{t^{\beta_k} - (\tnow)^{\beta_k}}{\lambda_k}}\Big]^{\cknow - l_k}
    \times \nonumber\\ & \hspace*{14.5ex}
    \frac{\big(\nknow\yknow\big)^{\nknow + 1}}{\Gamma(\nknow + 1)}
    \lambda_k^{-(\nknow + 1) - 1} e^{-\frac{\nknow\yknow}{\lambda_k}} \dd \lambda_k \nonumber\\
 &= { \cknow \choose l_k} \sum_{j=0}^{\cknow - l_k} (-1)^j { \cknow - l_k \choose j}
    \frac{\big(\nknow\yknow\big)^{\nknow + 1}}{\Gamma(\nknow + 1)} 
    \times \nonumber\\ & \hspace*{1ex}
    \int \lambda_k^{-(\nknow + 1) - 1}
    \exp\Big\{-\frac{(l_k + j) (t^{\beta_k} - (\tnow)^{\beta_k}) + \nknow\yknow}{\lambda_k}\Big\} \dd \lambda_k\,.
\end{align}
The terms remaining under the integral form the core of an inverse Gamma distribution \eqref{eq:ig-def}
with parameters $\nknow + 1$ and $\nknow\yknow + (l_k + j) (t^{\beta_k} - (\tnow)^{\beta_k}))$,
allowing to solve the integral using the corresponding normalization constant.
We thus have, for $l_k \in \{0,1,\ldots,\cknow\}$,
\begin{align}
\label{eq:postpred-priorparams}
\lefteqn{P(C^k_t = l_k\mid\nkz,\ykz, \vectknow)}\hspace*{5ex} \nonumber\\
 &= { \cknow \choose l_k} \sum_{j=0}^{\cknow - l_k} (-1)^j { \cknow - l_k \choose j} \times \nonumber\\ & \hspace*{10ex}
    \left(\frac{\nknow\yknow}{\nknow\yknow + (l_k + j) \big(t^{\beta_k} - (\tnow)^{\beta_k}\big)}\right)^{\nknow + 1} \,, \\
 & \quad\text{where } \nknow\yknow = \nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + \cknow (\tnow)^{\beta_k} \nonumber\\
 & \quad\text{and }  \nknow = \nkz + \eknow \,. \nonumber
% &= { \cknow \choose l_k} \sum_{j=0}^{\cknow - l_k} (-1)^j { \cknow - l_k \choose j} \times \nonumber\\ & \hspace*{1ex}
%    \left(\frac{\nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + \cknow (\tnow)^{\beta_k} }%
%               {\nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + \cknow (\tnow)^{\beta_k} +%
%                (l_k + j) \big(t^{\beta_k} - (\tnow)^{\beta_k}\big) }\right)^{\nkz + \eknow + 1}.
% &= \sum_{j=0}^{\cknow - l_k} (-1)^j \frac{\cknow !}{l_k! j! (\cknow - l_k - j)!}   
%    \left(\frac{\nknow\yknow}{\nknow\yknow + (l_k + j) \big(t^{\beta_k} - (\tnow)^{\beta_k}\big)}\right)^{\nknow + 1} \nonumber\\
% &= \sum_{j=0}^{\cknow - l_k} (-1)^j \frac{\cknow !}{l_k! j! (\cknow - l_k - j)!} \times \nonumber\\ & \hspace*{10ex}  
%    \left(\frac{\nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + \cknow       (\tnow)^{\beta_k} }%
%               {\nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + (\cknow - l_k - j) (\tnow)^{\beta_k} + (l_k + j) t^{\beta_k} }\right)^{%
%    \nkz + \eknow + 1}.
\end{align}
%where $\nknow\yknow = \nkz\ykz + \sum_{i=1}^{\eknow} (t_{k,i})^{\beta_k} + \cknow (\tnow)^{\beta_k}$
%and $\nknow = \nkz + \eknow + 1$.
%These posterior predictive probabilities can also be expressed as a cumulative probability mass function (cmf)
%\begin{align}
%F(l_k \mid \nkz,\ykz,\vectknow) = P(C^k_t \leq l_k \mid \nkz,\ykz,\vectknow) 
% = \sum_{j=0}^{l_k} P(C^k_t = j \mid \nkz,\ykz,\vectknow)\,.
%\end{align}


\subsection{Conditional system reliability function at $\tnow$}
\label{sec:sysreltnow}

Now that we can calulate the component posterior predictive distributions at $\tnow$,
we can use the method from Section~\ref{sec:sysrel}
to determine the system residual life distribution (RLD) $\Rsysnow(t)$,
giving us the probability that the system functions at times $t > \tnow$,
taking into account all information available at $\tnow$:
\begin{align}
\Rsysnow(t) &= \sum_{l_1=0}^{c_1^{(\tnow)}} \cdots \sum_{l_K=0}^{c_K^{(\tnow)}} \Phinow(l_1,\ldots,l_K)
               \prod_{k=1}^K P(C^k_t = l_k\mid\nkz,\ykz, \vectknow)\,.
\label{eq:sysrel-tnow}
\end{align}
Since this is the distribution of system reliability conditional on the system surviving until $\tnow$,
we have that $\Rsysnow(\tnow) = 1$ for any $\tnow$.
Note that if one or several components have failed by $\tnow$,
the system reliability block diagram changes, and with it the survival signature $\Phi(l_1,\ldots,l_K)$.
We denote the current survival signature by $\Phinow(l_1,\ldots,l_K)$.

For each prediction time $t$,
\eqref{eq:sysrel-tnow} is a sum over $\prod_{k=1}^K (\cknow + 1)$ terms.
However, some of these terms correspond to $\Phinow(l_1,\ldots,l_K) = 0$,
which can thus be disregarded.
For each of the remaining terms,
we must calculate the product $\prod_{k=1}^K P(C^k_t = l_k\mid\nkz,\ykz, \vectknow)$.
For all products, the constituting factors can be taken from the same table
enumerating $P(C^k_t = l_k\mid\nkz,\ykz, \vectknow)$ for all $l_k \in \{0, 1, \cknow\}$, $k=1\ldots,K$,
so \eqref{eq:postpred-priorparams} needs to be evaluated 
only $\sum_{k=1}^K (\cknow + 1)$ times.

We will evaluate \eqref{eq:sysrel-tnow} on a dense grid of prediction times $t > \tnow$,
thus discretely approximating the RLD.
As the evaluation of \eqref{eq:sysrel-tnow} for each $t$ does not involve any complex numeric calculations or Monte Carlo sampling,
the grid of prediction values can be very fine.
%
%Next, we will describe an adaptive maintenance policy based on %\eqref{eq:sysrel-tnow}
%a current residual life distribution,
%first in general, then for our implementation,
%where a fine discrete approximation of the RLD is obtained.


\section{Dynamic and adaptive maintenance policy based on the current system residual life distribution}
\label{sec:policy}

Now we can derive the adaptive and dynamic maintenance policy
that resembles a CBM policy.
Consider a set of evaluation time points $\tnow$.
Instead of determining a threshold for a degradation signal like in usual CBM approaches,
we determine, for each present time $\tnow$,
the adaptive and dynamic time span $\tausnow$ which gives the time from $\tnow$ until the optimal moment of maintenance.
Close to system startup, $\tausnow$ will be large;
but as components in the system age and some indeed fail over time,
$\tausnow$ will typically decrease.
%Let $\tprep$ be the time span needed to prepare maintenance work.
%Let $\tthresh$ be a threshold time span chosen for practical reasons.
%used to decide when maintenance is initiated, i.e.,
A preventive maintenance action is initiated
as soon as $\tausnow$ decreases below the time span to the next evaluation time.
%continuing to the the next evaluation point $\tnow$ would carry the risk 
%a time span chosen to reflect operational practicalities.
This dynamic and adaptive maintenance policy is described in more detail in Section~\ref{sec:operationalprocedure};
a flow chart for the policy is given in Figure~\ref{fig:procedure}.

The optimal time to maintenance $\tausnow$ is determined based on $\Rsysnow(t)$
by minimizing $\gnow(\tau)$, the expected cost rate for the current operational cycle.
An operational cycle begins with system start-up,
and ends when either preventive maintenance is carried out,
or a failure of the system occurs, with subsequent corrective maintenance.
This criterion, as detailed in Section~\ref{sec:costrate},
%only assumes that $\Rsysnow(t)$ is given,
only assumes that a RLD is given,
and thus is not tied to our choice for the component models.
One could also consider alternative optimality criteria to determine $\tausnow$;
when safety is paramount one could, e.g., determine $\tausnow$ such that
the probability of system failure is at most at some very low pre-determined level.

Section~\ref{sec:optim} then focuses on our choice for the component models and the resulting form for $\Rsysnow(t)$,
and shows how $\tausnow$ is determined in this situation.
%Numerical examples comparing results for three exemplary failure histories are given in Section~\ref{sec:examples},
%and a small simulation study is presented in Section~\ref{sec:sim}.

\begin{figure}
\centering
\begin{tikzpicture}
[node distance=6mm,
 font=\footnotesize,
 box/.style={draw, rectangle, thick, minimum height=6mm, minimum width=15mm},
 box2/.style={draw, rectangle, thick, minimum height=6mm, minimum width=28mm, rounded corners=2mm},
 dec/.style={draw, shape=diamond, thick, minimum height=4mm, minimum width=4mm},
 pfeil/.style={-latex', very thick}]
\node (startup) [box] {\parbox{18ex}{\centering system start-up\\ $\tnow = 0, m= 0$\\ initial params.\\ $\nkz, \ykz$}};
\node (mincrem) [box, below=57mm of startup] {$m = m + 1$};
\node (cptfail) [dec, below right=9mm and 17mm] {\parbox{10ex}{\centering $\exists\, t_{k,i}$ s.t.\\ \rule{0ex}{2ex} \\ ?}};
%$(m-1)\delta$\\ $ < t_{k,i} < $\\ $m\delta$}};
\node at (cptfail) {$\scriptstyle \tnow < t_{k,i} < (m+1)\delta$};
%\coordinate (mcoord) at ($(startup) * (0, -2)$) {};
\node (nfltnow) [box, below=of cptfail] {\parbox{21ex}{\centering $\tnow = (m + 1)\delta$,\\ update parameters\\ to $\nknow, \yknow$}};
\node (yfltnow) [box, right=of cptfail] {\parbox{21ex}{\centering $\tnow = t_{k,i}$,\\ update parameters\\ to $\nknow, \yknow$}};
\node (sysfail) [dec, below=5.6mm of yfltnow] {\parbox{8ex}{\centering system\\ failed?}};
\node (tausmde) [box, below=of nfltnow] {\parbox{21ex}{\centering determine $\tausnow$}};
\node (taustki) [box, below=of sysfail] {\parbox{21ex}{\centering determine $\tausnow$}};
\node (tausdel) [dec, below=of tausmde] {\parbox{10ex}{\centering $\tausnow$\\ \rule{2ex}{0ex}$< \delta$?}};
\node (taudtki) [dec, below=of taustki] {\parbox{10ex}{$\tausnow <$\\ \phantom{?}}};
\node at ($(taudtki) + (0,-2mm)$) {$\scriptstyle (m + 1) \delta - t_{k,i}$?};
\node (corrmnt) [box, right=8.5mm of sysfail] {\parbox{21ex}{\centering corrective repair\\ at $\tnow$ (cost $c_c$)}};
\node (prevmnt) [box, right=of taudtki] {\parbox{21ex}{\centering preventive repair\\ at $\tnow$ (cost $c_p$)}};
\node (updatep) [box, below=7.5mm of corrmnt] {\parbox{21ex}{\centering set initial params.\\ for next cycle as\\ $\nkz = \nknow$\\ $\ykz = \yknow$}};
%\coordinate (nocoord) at ($(taudtki) + (-1.8,-1.3)$) {};
\coordinate (nocoord) at ($(taudtki) + (-1.8, 1.6)$) {};
\coordinate (yscoord) at ($(tausdel) + ( 0  ,-1.5)$) {};
\coordinate (nxtcoor) at ($(updatep) + ( 1.8, 0  )$) {};
\draw[pfeil] (startup) |- ($(cptfail.west) + (0.1, 0.1)$);
\draw[pfeil] (mincrem) |- ($(cptfail.west) + (0.1,-0.1)$);
\draw[pfeil] (cptfail) -- (nfltnow) node[midway, left] {no};
\draw[pfeil] (cptfail) -- (yfltnow) node[midway, above] {yes};
\draw[pfeil] (yfltnow) -- (sysfail);
\draw[pfeil] (nfltnow) -- (tausmde);
\draw[pfeil] (sysfail) -- (taustki) node[midway, left] {no};
\draw[pfeil] (sysfail) -- (corrmnt) node[midway, above] {yes};
\draw[pfeil] (tausmde) -- (tausdel);
\draw[pfeil] (taustki) -- (taudtki);
\draw[pfeil] (tausdel) -| (mincrem) node[at start, above left] {no};
\draw[very thick] (taudtki) -| (nocoord) node[near start, above] {no} -| (mincrem);
\draw[pfeil] (taudtki) -- (prevmnt) node[midway, above] {yes};
\draw[pfeil] (tausdel) -- (yscoord) -| (prevmnt) node[at start, above right] {yes};
\draw[pfeil] (prevmnt) -- (updatep);
\draw[pfeil] (corrmnt) -- (updatep);
\draw[pfeil, dotted] (updatep) -- (nxtcoor) |- (startup) node[midway, above left] {start new cycle};
%
%\node (startup) [box]                       {\parbox{20ex}{\centering system start-up\\ (all comp.\ new)}};
%\node (sysfail) [box, right=of startup]     {system failed?};
%\node (tauchck) [box, below=6mm of sysfail] {$\tausnow < \delta$?};
%\node (gotonxt) [box, below=6mm of tauchck] {\parbox{17ex}{\centering continue to\\ $\tnow = (m+1)\delta$}};
%\draw [thick, dashed, rounded corners=2mm] ($(gotonxt.south east) + (2mm,-1mm)$) rectangle ($(sysfail.north west) + (-5mm,1mm)$) node[above left=0mm and -33mm] {each $\tnow = m \delta, m \in \naturals_0$};
%\node [above left=0mm and -8mm of startup] {$t=0$};
%\node (correct) [box, right=of sysfail, minimum width=18mm] {\parbox{14ex}{\centering corrective\\ maintenance\\ (cost $c_c$)}};
%\node (prevent) [box, right=of tauchck, minimum width=18mm] {\parbox{14ex}{\centering preventive\\ maintenance\\ (cost $c_p$)}};
%\node (update) [box, below right=10mm and -4mm of prevent, minimum width=10mm] {\parbox{12ex}{\centering update parameters}};
%\draw[pfeil] (startup) -- (sysfail);
%\draw[pfeil] (sysfail) -- (tauchck) node[midway, right] {no};
%\draw[pfeil] (tauchck) -- (gotonxt) node[midway, right] {no};
%\draw[very thick] (gotonxt.west) -| ($(sysfail.west) + (-3mm,0mm)$);
%\draw[pfeil] (sysfail) -- (correct) node[midway, above] {yes};
%\draw[pfeil] (tauchck) -- (prevent) node[midway, above] {yes};
%\draw[pfeil] (correct) -| (update);
%\draw[pfeil] (prevent) -| (update);
%\draw[pfeil, dotted] (update) -| (startup) node[midway, below right] {start new cycle};
\end{tikzpicture}
\caption{Flowchart for the proposed maintenance policy,
showing a single operational cycle,
including the loop back to system startup that starts a new cycle as a dashed arrow.}
%For simplicity of presentation, we exclude the possibility that several component failure times
%happen between the evaluation times $(m-1) \delta$ and $m \delta$.}
\label{fig:procedure}
\end{figure}

\subsection{Detailed description of the policy}
\label{sec:operationalprocedure}

Each operational cycle starts with a system startup.
At this point in time, the system is in an (as good as) new state,
i.e., all components in the system are (as good as) new and have not aged.
An operational cycle ends when the system is either maintained preventively, %at time $\tnow + \tausnow$ (on an absolute time scale),
or the system fails, which triggers a corrective maintenance action.

Define a grid of planned evaluation time points as $\{m \delta : m \in \naturals_0\}$,
%where $m = 0, 1, 2, \ldots$,
%and $\delta$ sufficiently small to be negligible on the system lifetime scale.
with $\delta$ a sufficiently small time increment.
During an operational cycle,
$\tausnow$ is usually recalculated at times $\tnow = m \delta$, $m = 0, 1, 2, \ldots$.
%where the time increment $\delta$ should be small enough to be negligible on the scale of system lifetime.
However, if a component fails before the next $\tnow$ on the grid,
$\tnow$ is set instead to the exact failure time $t_{k,i}$.
Next, we check whether the system has failed due to this component failure.
If the system has failed, a corrective maintenance action is carried out, incurring cost $c_c$.
If the system has not failed, we calculate $\tausnow$ is for the current off-grid time $\tnow = t_{k,i}$,
and move back onto the grid afterwards,
unless there is another component failure before the next planned evaluation time.

Frequent recalculation of $\tausnow$ is feasible
when computation of the RLD is fast.
Then, by evaluating the unit cost rate function $\gnow(\tau)$ on a discrete grid,
$\tausnow$ can be approximated with a simple grid search.
%because we use a fast discrete approximation of the unit cost rate function $\gnow(\tau)$,
%and determine $\tausnow$ with a simple grid search.
Updating $\tausnow$ frequently is useful because,
even in absence of failures of system components,
all components age, and also non-failure adds information to the component models.
%Both aspects influence $\Rsysnow(t)$ and thus $\tausnow$,
%as is visible in the top panel of Figure~\ref{fig:tauhist1fig2T},
%which gives a numerical example described in more detail in Section~\ref{sec:examples}.
%An alternative would be to update $\tausnow$ only when a component fails,
%but as a (near) real-time update of $\tausnow$ is possible and useful,
%as non-failed components still can be used to update component models ($\nknow$ and $\yknow$) via right-censored observation $\tpnow$.
%
The calculation of $\tausnow$ for a fixed $\tnow$ resembles the calculation
of the maintenance interval in an age based policy by trading off the costs of preventive and corrective maintenance.
However, because $\tausnow$ is derived from the current $\Rsysnow(t)$,
our policy is adaptive and dynamic like a CBM policy. 
%As can happen for an age-based policy, the derived $\gnow(\tau)$ may also be monotonely decreasing.
%In this case, it is, at the moment $\tnow$, optimal to correctively maintain the system.
%This means that our approach is not only capable of finding the optimal moment for preventive maintenance,
%but also capable of indicating when corrective maintenance is the better option.
%i.e., capable of choosing the maintenance policy.

Once $\tausnow$ is calculated, it is compared to the time span from the current $\tnow$ to the next planned evaluation time.
If the current $\tnow$ is a time on the grid, then $\tnow = m \delta$ for some $m \in \naturals_0$.
The next planned evaluation is then at $(m + 1) \delta$,
and we compare $\tausnow$ to $\delta$.
If $\tausnow < \delta$, a preventive maintenance action is triggered, incurring cost $c_p$,
otherwise we attempt to move to the next planned evaluation point, by setting $m =  (m + 1)$.

If the current $\tnow$ is instead an off-grid time corresponding to a component failure time $t_{k,i}$,
then the next planned evaluation time is the next time on the grid.
Let $m$ be such that $m \delta < t_{k,i} < (m + 1) \delta$,
so the next time on the grid is $(m + 1) \delta$,
and we compare $\tausnow$ with $(m + 1) \delta - t_{k,i}$.
If $\tausnow < (m + 1) \delta - t_{k,i}$, a preventive maintenance action is triggered, incurring cost $c_p$,
otherwise we attempt to move to the next planned evaluation point, which is still $(m + 1) \delta$.

In both cases, we decide to maintain the system preventively
when the optimal time to maintenance is closer to now than the next time we intend to re-evaluate $\tausnow$,
since otherwise, in waiting to the next evaluation time,
one would exceed the optimal system failure risk 
as determined by the trade-off made by $\gnow(\cdot)$.
When $\tausnow$ is instead further in the future than the next planned evaluation time,
we can safely continue to the next evaluation time,
without risking to miss the cost-optimal moment to preventively maintain the system.

At the end of each operational cycle, i.e.,
after maintaining the system either preventively or correctively,
the component information gained during the completed operational cycle is added to the component knowledge base
by updating the parameters $\nk$ and $\yk$ accordingly,
%using failure and censoring times as of the end of the cycle.
such that $\nkz$ and $\ykz$ for the next operational cycle
account for the (censored) lifetimes observed during the previous operational cycle.
%We assume that both planned and unplanned maintenance action takes negligible time,

Figure~\ref{fig:procedure} displays the flowchart for the policy,
indicating the decisions that are repatedly encountered in a single operational cycle.
The flowchart also contains the loop back to system startup that starts a new operational cycle.

We assume that both preventive and corrective maintenance result in an (as good as) new system state by replacing all components,
such that the system begins a new operational cycle always under the same conditions.
Replacement schemes that do not replace all components would require to account for the different ages of components in the system
at the start of a new cycle.
While this is in principle possible in our approach,
exploring such selective replacement schemes is out of scope for this paper,
since optimizing over replacement schemes is, in our view, a higly interesting topic
that merits an investigation of its own,
and we will comment on this in Section~\ref{sec:outlook}.
%
%As we use a one-cycle criterion, the time both preventive and corrective maintenance actions require
%does not matter directly for the determination of $\tausnow$.
%Longer downtime and other negative effects of corrective maintenance
%are instead accounted for through the costs parameters $c_p$ and $c_c$ used in the trade-off leading to $\tausnow$,
%as described next.


\subsection{Expected operational cycle cost rate}
\label{sec:costrate}

To determine the optimal time to maintenance $\tausnow$,
we use the expected one-cycle cost rate 
\citep{1984:ansell-bendell-humble,1996:mazzuchi-soyer,2006:coolen-schrijner-coolen}
as the unit cost rate.
Although a different, renewal theory based way to calculate the unit cost rate is often used
in CBM literature \citep[e.g.,][]{2013:si-et-al,2011:kim-et-al},
%As we discussed in the Introduction,
we use the one-cycle cost rate as it is the most appropriate for our situation,
where $\tausnow$ changes continuously.

Let $c_p$ be the cost of preventive maintenance, and $c_c$ the cost of corrective (unplanned, breakdown) maintenance, where $c_p < c_c$.
Usually, set-up costs, the actual maintenance costs (labor, new components) and downtime costs are much higher for corrective maintenance,
such that often, $c_p \ll c_c$.

Let $\Tsysnow$ be the random variable corresponding to $\Rsysnow(t)$,
giving the (random) system failure time for times $t > \tnow$,
conditioned on all information available at $\tnow$,
and $\fsysnow(t)$ be the corresponding probability density.
Note that due to the conditional nature of $\Rsysnow(t)$,
which gives the probability that the system functions at time $t$ given that it functions at $\tnow$,
also $\fsysnow(t)$ is a conditional probability density,
with support $\{t \in \reals : t > \tnow \}$.
%In particular, we have $\fsysnow(t) = 0$ for all $t \le \tnow$.

Conditional on the realization $\tsysnow$ of $\Tsysnow$, the unit cost rate is 
\begin{align}
g(\tau \mid \Tsysnow = \tsysnow) &=
\begin{cases}
%c_p / \tau               & \text{if } \tsysnow \ge \tnow + \tau \\
%c_u / (\tsysnow - \tnow) & \text{if } \tnow < \tsysnow  <  \tnow + \tau \,.
c_p / (\tnow + \tau) & \text{if } \hspace{7.4ex} \tsysnow \ge \tnow + \tau \\
c_c /  \tsysnow      & \text{if }        \tnow < \tsysnow  <  \tnow + \tau \,.
\end{cases}
\label{eq:gtau}
\end{align}
Note that here, $\tau$ is a time span starting from $\tnow$,
and so is a time on a prospective time scale, where $0$ corresponds to $\tnow$.
In contrast, $\tnow$ and $\tsysnow$ are times on an absolute time scale,
where $0$ corresponds to the time of system start-up.
Taking the expectation over $\Tsysnow$ in \eqref{eq:gtau} leads to the expected operational cycle cost rate
\begin{align}
\gnow(\tau) &= \E[g(\tau \mid \Tsysnow)] \nonumber\\ 
            &= \frac{c_p}{\tnow + \tau} \Rsysnow(\tnow + \tau)
              + c_c \int_0^\tau \frac{1}{\tnow + \theta} \fsysnow(\tnow + \theta) \dd \theta\,.
\label{eq:gtnowtau}
\end{align}
Here, we assume that the time required to execute preventive and corrective maintenance
can be neglected when compared with the duration of one operational cycle.
%
$\gnow(\tau)$ gives the cost rate that we expect to incur over the complete operational cycle
when, at time $\tnow$, we schedule preventive maintenance for time $\tnow + \tau$,
so $\gnow(\tau)$ trades off the risk of system failure before $\tnow + \tau$
with the gains we could reap when the system survives until $\tnow + \tau$.
Figure~\ref{fig:ghist1fig2T} shows $\Rsysnow(t)$ and the corresponding $\gnow(\tau)$ for a number of current times $\tnow$
in a numeric example which is described in more detail in Section~\ref{sec:examples}.

\begin{figure}
\includegraphics[width=\textwidth]{ghist1fig2T}
\caption{System reliability and unit cost rate functions for $\tnow = 0,2,4,6,8$
for the system depicted in Figure~\ref{fig:brakesys-layout}
and prior component models as given in Table~\ref{tab:priorparams},
assuming the failure history $P_2 = 3$, $P_3 = 4$, $C_2 = 6$, $C_3 = 7$, $H = 8$,
and costs $c_u = 1$, $c_p = 0.2$.}
\label{fig:ghist1fig2T}
\end{figure}

Note that for the integral in \eqref{eq:gtnowtau} to exist,
%$\E[1/\Tsysnow \mid \Tsysnow > \tnow]$ must exist,
$\E[1/(\Tsysnow - \tnow)]$ must exist,
which is a condition for $\fsysnow(\tnow + \theta)$ for values of $\theta$ close to $0$ \citep{2006:coolen-schrijner-coolen}.
%This restriction probably plays a role in the CBM literature's reluctance to adopt the one-cycle criterion.
For our approach, however, this condition is not relevant,
as we use a discrete approximation of $\Rsysnow(t)$ and $\fsysnow(t)$, respectively
(see Section~\ref{sec:optim} below).

$\tausnow := \arg\min \gnow(\tau)$
is the cost-optimal time to maintenance as of $\tnow$,
incorporating all that is known at current time $\tnow$.
%The lower index $\tnow$ of $\tausnow$ emphasizes that
%depends on $\tnow$, and also on the history of the system.
%illustrates the derivation of $\tausnow$ .
%Minimizing $\gnow(\tau)$ leads to $\tausnow$,
%the optimal moment of maintenance on a prospective time scale.
The minimal expected operational cycle cost rate $\gstarnow := \gnow(\tausnow)$ corresponding to $\tausnow$
thus is, as of $\tnow$, the lowest cost rate for the current operational cycle
that we can expected to attain.
%attainable for the span of the remaining lifetime of the system,
%i.e., for what happens beyond current time $\tnow$.
%$\gstarnow$ increases over time since 
%***Also of interest is the total expected cost rate as of $\tnow$, denoted by $\gtotalnow$.
%This gives the expected unit cost rate calculated over the full time since system start-up,
%under the assumption that maintenance is scheduled for $\tstarnow := \tnow + \tausnow$.
$\gstarnow$ generally decreases with $\tnow$ since for larger $\tnow$, maintenance costs are spread over a longer time period.
However, when many (or few but critical) components fail, $\gstarnow$ may increase,
since then, system failure becomes much more likely, leading to a higher risk of incurring the corrective maintenance cost $c_c$.
Figure~\ref{fig:tauhist1fig2T} illustrates these relations for the scenario depicted in Figure~\ref{fig:ghist1fig2T}.
%
As can be seen there and in the numerical examples in Section~\ref{sec:examples},
for continuous component lifetime distributions,
$\tausnow$ is a smooth function of $\tnow$ during periods where no component fails,
with discrete jumps at component failure times.
The size of the jump depends on the relevance of the failed component for the current system reliability.
Common cause failures, i.e., failures of several components at the same time due to a shared root cause,
are currently not accounted for in our model. %, as we assume component failures to be mutually independent
%given the component model parameters.
Such simulatenous failures will seriously influence the RLD and thus $\tausnow$.
We will return to this in Section~\ref{sec:outlook}.
%
%Next, we describe how to numerically evaluate \eqref{eq:gtnowtau} and determine $\tausnow$
%for our choice of component model.

\begin{figure}
%\centering
\includegraphics[width=\textwidth]{tauhist1fig2T}
\caption[t]{$\tausnow$, $\tstarnow = \tnow + \tausnow$ and $\gstarnow = \gnow(\tausnow)$
%and the total unit cost rate $\gtotalnow$ as functions of $\tnow$
for the system depicted in Figure~\ref{fig:brakesys-layout},
assuming the failure history $P_2 = 3$, $P_3 = 4$, $C_2 = 6$, $C_3 = 7$, $H = 8$,
and $c_u = 1$, $c_p = 0.2$.}
\label{fig:tauhist1fig2T}
\end{figure}


\subsection{Numerical optimization of the expected operational cycle cost rate}
\label{sec:optim}

For our choice of the component models,
the resulting $\Rsysnow(t)$ as of \eqref{eq:sysrel-tnow}
is closed-form, but not tractable analytically.
Therefore,
we evaluate $\Rsysnow(t)$ on a dense grid of time points $t$
covering the time span from $\tnow$ to some horizon time $\tnow + t_h$.
%(The choice of $t_h$ is discussed in more detail below.)
We calculate $\gnow(\tau)$ on the same grid,
and determine $\tausnow$ by minimizing over this grid.
Here, we approximate the integral in \eqref{eq:gtnowtau} numerically by a sum,
where $\fsysnow(t)$ is calculated via differences of $\Rsysnow(t)$.
%$\gnow(\tau)$ is likewise evaluated on a dense grid,
%and $\tausnow = \arg\min \gnow(\tau)$ is determined as the minimum over the grid.

The horizon time span $t_h$ can be determined by practical considerations,
but generally should reach far enough into the tail of $\Rsysnow(t)$
such that $\gnow(\tau)$ is not cut off before it reaches its minimum. %one can be sure to find $\tausnow$.
Note that according to \eqref{eq:gtnowtau},
the calculation of $\gnow(\tau)$ requires
evaluations of $\Rsysnow(t)$ and $\fsysnow(t)$ only for $\tnow < t \le \tnow + \tau$.
The choice of $t_h$ is thus not critical,
in the sense that $\gnow(\tau)$ evaluations will be correct even if $t_h < \tausnow$.

We have implememted our method in \textsf{R} \citep{R},
using the package \texttt{ReliabilityTheory} \citep{2016:aslett-RT}
to calculate survival signatures.
As the parameter update step is closed-form,
recalculation of $\tausnow$ is a matter of one or two seconds,
allowing for a near real-time update.
%
%***calculations in \textsf{R} can be vectorized and are thus fast.
%***if min $\gnow(\tau)$ at $\tnow + t_h$, corrective maintenance is optimal: policy switch!
%***if $\tau_*^{(\tnow)}$ larger than certain horizon, than conclude corrective policy


\section{Further elaboration of the case study}
\label{sec:examples}

We now explain in more detail the case study used for the previous illustrative figures.
We use the simplified automotive brake system as introduced in Section~\ref{sec:sysrel},
see Figure~\ref{fig:brakesys-layout} for the reliability block diagram.

The cost parameters are chosen as $c_c = 1$, $c_p = 0.2$, % in all examples,
i.e., corrective maintenance is five times more costly than preventive maintenance.
This makes it worthwile to aim for a preventive repair of the system,
but corrective maintenance will not be avoided at all cost.
%Indeed, $c_u / c_p = 5$ means that risking corrective maintenance is worthwile
%if one can expect to run the system five times longer
%than the preventive maintenance interval.

The choice of prior parameters for the four component models are discussed in Section~\ref{sec:ex-prior}.
Then, our maintenance policy is illustrated under three different exemplary failure time histories,
described in Sections~\ref{sec:ex-1}, \ref{sec:ex-2} and \ref{sec:ex-3}.

\begin{table}
\centering
\begin{tabular}{crrrrrrr}
  \toprule
$k$ & $\beta_k$ & $\E[T_i^k]$ & $\ykz$ & $\nkz$ \\
  \midrule
% test prior
%M & $2.5$ & $5$\rule{1.5ex}{0ex} & $75.4$ & $2$\rule{1ex}{0ex} \\
%H & $1.2$ & $2$\rule{1.5ex}{0ex} & $ 2.5$ & $1$\rule{1ex}{0ex} \\
%C & $2  $ & $8$\rule{1.5ex}{0ex} & $81.5$ & $1$\rule{1ex}{0ex} \\
%P & $1.5$ & $3$\rule{1.5ex}{0ex} & $ 6.1$ & $1$\rule{1ex}{0ex} \\
% prior 1
C & $2  $ & $ 3$\rule{1.5ex}{0ex} & $ 11.5$ & $1$\rule{1ex}{0ex} \\
H & $1  $ & $10$\rule{1.5ex}{0ex} & $ 10.0$ & $1$\rule{1ex}{0ex} \\
M & $2.5$ & $ 8$\rule{1.5ex}{0ex} & $244.1$ & $1$\rule{1ex}{0ex} \\
P & $1.5$ & $ 5$\rule{1.5ex}{0ex} & $ 13.0$ & $1$\rule{1ex}{0ex} \\
  \bottomrule
\end{tabular}
\caption{Prior parameters for the four component types as used in the numerical examples.}
%The corresponding expected failure behaviour is visualized in Figure~\ref{fig:compprior1fig1}.}
\label{tab:priorparams}
\end{table}


\subsection{Prior component models}
\label{sec:ex-prior}

The prior component model parameters chosen for this case study
are given in Table~\ref{tab:priorparams}.
Note that the scale parameter for component type H is $1$,
such that the failure times for this component type are assumed to follow an Exponential distribution.
In cases where only $H$ and one or both of $P_3$ and $P_4$ are functioning,
the system reliability function will thus be close to exponential,
making preventive maintenance unattractive.

Note also that $\nkz = 1$ for all four component types,
indicating very weak prior knowledge.
Observed failure times thus have a strong influence on the updating of $\ykz$,
such that component models are primarily data-driven.
This offers the advantage of quick adaption
for the case that an expert's assessment of the component MTTF turns out to be inadequate.
%things could look different for stronger expert information!
When $\nkz$ and $\ykz$ also include test data,
then $\nkz$ will typically be larger,
since the update of $n_k$ involves the number of observed failures
(see Section~\ref{sec:weibull}).

The expected component failure behaviour obtained by the parameter choices in Table~\ref{tab:priorparams}
is visualized in Figure~\ref{fig:compprior1fig1},
showing the prior predictive reliability function for a single component of each type.
Note that these functions are not plain Weibull reliability functions,
but weighted averages over Weibull reliability functions according to the prior distribution over the scale parameter $\lambda_k$.
We see that, e.g., the median prior expected failure times are
$2.2$, $4.1$, $6.3$, and $3.1$ for type $C$, $H$, $M$, and $P$ components, respectively.
While it is a priori quite unlikely that any type $C$ component will survive past time $6$,
we expect 25\% of type $H$ components to survive past time $10$.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{compprior1fig1}
\caption{Prior predictive reliability functions corresponding to the values from Table~\ref{tab:priorparams},
%for the four component types of the system depicted in Figure~\ref{fig:brakesys-layout},
showing the expected failure behaviour.}
\label{fig:compprior1fig1}
\end{figure}


\subsection{Failure history 1: failures later than expected}
\label{sec:ex-1}

In the first failure history, we observe
$P_2 = 3$, $P_3 = 4$, $C_2 = 6$, $C_3 = 7$ and $H = 8$,
and in following the system until $\tnow = 8$,
all other failure times are right-censored.
This is the failure history already used for Figures~\ref{fig:ghist1fig2T} and \ref{fig:tauhist1fig2T}.
Here, failures observed for type $C$ are thus considerably later than expected,
while for type $P$, the two observed failures are close to the assumed MTTF. 
Both failure at time $8$ for $H$ and non-failure by time $8$ for $M$ are nothing unusual.
In total, due to the late type $C$ failures, prior assumptions turn out to be somewhat pessimistic.

This is reflected in Figure~\ref{fig:tauhist1fig2T},
where $\tausnow$ (left panel) first decreases quickly but then tends to decrease more slowly during failure-free periods,
which is a consequence of adjusting prior expectations for type $C$ components
due to their unexpected reliability.
Large drops of $\tausnow$ happen only at the failures of components $P_2$, $P_3$ and $H$
at times 3, 4 and 8, respectively,
as $\Rsysnow(t)$ changes considerably at these moments.
$\tausnow$ falls below $\delta = 0.1$ at time $\tnow = 6$,
triggering preventive maintenance of the system.
$\tausnow = 1.60$ at $\tnow = 0$, this corresponds to the optimal maintenance time
in an age-based policy that takes the system as monolithic. 
Since $\tausnow$ does not change dramatically most of the time, $\tstarnow$,
the cost-optimal moment of maintenance on the absolute timescale (center panel),
steadily increases, with small drops corresponding to drops in $\tausnow$,
showing that as time progresses, accumulated information indicates that maintenance can be postponed.
Consequently, $\gstarnow$ (right panel) is steadily decreasing,
with small upward jumps at the failures of $P_2$ and $P_3$.
%so the drops in $\tausnow$ do not entirely outweigh the higher likelihood of system failure
%due to the failure of these components.
%\textbf{***show $\Rsysnow(\tausnow)$???}
At $\tnow = 6$, the expected operational cycle cost rate is $0.033$,
and so considerably lower than the operational cycle cost rate of $0.2 / 1.6 = 0.125$
that one obtains by maintaining the system preventively at time 1.6,
as suggested by the age-based policy that disregards the information from component monitoring.
In this example, using the component failure information allows to lower the cost rate by 73\%.

\begin{figure}
\includegraphics[width=\textwidth]{tauhist1fig3T}
\caption{Illustration of the gains due to the parameter update at each $\tnow$,
assuming the failure history $P_2 = 3$, $P_3 = 4$, $C_2 = 6$, $C_3 = 7$, $H = 8$.}
\label{fig:tauhist1fig3T}
\end{figure}

The effect of the continuous parameter update in our policy is illustrated in Figure~\ref{fig:tauhist1fig3T}.
There, the results as seen already in Figure~\ref{fig:tauhist1fig2T} are compared
to the case when component parameters are not updated during the operation of the system,
while still accounting for the fact that components age,
and that a failed component changes the system layout.
%***so dynamic but not adaptive?***
We see that $\tausnow$ (left panel) is decreasing more quickly in the no update case,
with the jumps at component failure times being less pronounced than in the update case.
Without the continuous update, residual system lifetime continues to be underestimated,
and maintenance is scheduled too early,
%$\tausnow$ (left panel) is consistently lower for the non-adaptive algorithm,
%same with $\tstarnow$ (center panel).
%This is because the adaptive algorithm recognizes that system components are more reliable than expected,
leading to lower expected operational cycle cost rates $\gstarnow$ (right panel).
$\tausnow$ for the policy without parameter update drops below $\delta = 0.1$ at $\tnow = 4$,
while maintenance is triggered for the algorithm with parameter update at $\tnow = 6$.
The parameter update thus allows to run the system for two more time units,
corresponding to a unit cost rate of $0.2 / 6 = 0.033$ instead of $0.2 / 4 = 0.05$,
a reduction of 33\%.


\subsection{Failure history 2: failure times as expected}
\label{sec:ex-2}

In a second failure history, we observe the failure times $C_2 = 1$, $C_2 = 3$, $P_2 = 0.5$, $P_3 = 1.5$,
and follow the system until $\tnow = 2.5$.
Here, components of type $C$ behave more or less as expected,
while type $P$ components fail slightly earlier than expected.
Overall, prior assumptions are reasonably in line with observed failure behaviour.
%with a slight tendency to earlier failures.
Consequently, the difference between the continuous update case and the no update case is small,
as visualized in Figure~\ref{fig:tauhist2fig3T}.
There, $\tausnow$ values for the policy with parameter updates vary around those obtained for the policy without parameter updates,
and a similar behavior for $\tstarnow$ and $\gstarnow$ is observed.
With $\delta = 0.1$, preventive maintenance is carried out at times $2.6$ and $2.5$ for the two policies, respectively.
This shows that for this failure history, in which there is not much to be gained by the parameter update,
$\tausnow$ behaviour is dominated by the update of the system reliability block diagram.
Here, any differences due to the update are very small in comparison to the gains offered by the continuous update
in case of the first failure history
(note the differing vertical and horizontal scales in Figures~\ref{fig:tauhist1fig3T} and \ref{fig:tauhist2fig3T}).

\begin{figure}
\includegraphics[width=\textwidth]{tauhist2fig3T}
\caption{Effect of the continuous parameter update for the second failure history ($C_2 = 1$, $C_2 = 3$, $P_2 = 0.5$, $P_3 = 1.5$)
which is more or less in line with expectations.
%containing slightly early failures of type $P$ components.
With observed failure behaviour more or less in line with expectations,
both the policies with and without continuous update perform similarly,
unlike in case of the first failure history as depicted in Figure~\ref{fig:tauhist1fig3T},
where updating gave considerable gains.}
\label{fig:tauhist2fig3T}
\end{figure}


\subsection{Failure history 3: failures earlier than expected}
\label{sec:ex-3}

%The third failure history contains very late failures for all component types
%by assuming $C_2 = 10$, $C_3 = 12$, $H = 14$, $P_2 = 8$, $P_3 = 9$,
%and following the system until $\tnow = 14$.
%The late failure times for type $C$ components are especially surprising,
%with the prior predictive reliability being $0.01$ at time $10$
%(see top left panel of Figure~\ref{fig:compprior1fig1}).
%Here, the situation is more extreme than in the first failure history.
%Figure~\ref{fig:tauhist3fig3} shows that, after a first short dip,
%$\tausnow$ for the continuous update model steadily increases until the first observed failure at $\tnow = 8$.
%For the model without update, $\tausnow$ steadily decreases instead,
%ignoring any information on component behaviour to be gained during the current system run.
%Assuming $\tthresh = 0.5$, the method without parameter update would recommend to maintain the system preventively at $\tnow = 8.4$,
%with only $P_2$ having failed by then, and all other components up and running,
%corresponding to a considerable waste of component lifetime.
%
%\begin{figure}
%\includegraphics[width=\textwidth]{tauhist3fig3}
%\caption{Effect of the continuous parameter update for the third failure history,
%containing very late failures for all component types ($C_2 = 10$, $C_3 = 12$, $H = 14$, $P_2 = 8$, $P_3 = 9$).}
%\label{fig:tauhist3fig3}
%\end{figure}
%
The third failure history contains very early failures
with $C_2 = 0.1$, $C_3 = 0.2$, $C_4 = 0.8$ $H = 1$, $P_2 = 0.3$, $P_3 = 0.4$,
and we follow the system until $\tnow = 1$,
see Figure~\ref{fig:tauhist4fig3T}.

The policy without parameter update leads to consistently higher $\tausnow$ values,
while the policy with parameter update takes the surprisingly early failures into account.
Unlike in the other two failure histories, $\tstarnow$ is generally decreasing,
indicating that maintenance should happen much earlier than expected initially.
The policy with parameter update consistently suggests to maintain earlier than the policy without update,
but for both models, $\tausnow$ drops below $\delta = 0.1$ upon the failure of H at $\tnow = 1$,
obtaining an operational cycle cost rate of $0.2$.
In contrast, 
the age-based policy that disregards the information from component monitoring
would have scheduled maintenance for time $1.6$,
but it is very unlikely that the system will still function by that time.
If the system fails between time 1 and time 1.6,
then the age-based policy would lead to an operational cycle cost of $0.625$ to $1$,
much higher than for our policy.
%expected cycle costs are underestimated without update***

Like in failure history 2, the structural information (which components still function?)
seems to deliver a major part of the gains for our CBM policy,
whereas the parameter update seems most effective in situations where
component failures happen later than expected.
These findings are confirmed by results from a small simulation study described next.

\begin{figure}
\includegraphics[width=\textwidth]{tauhist4fig3T}
\caption{Effect of the continuous parameter update for the third failure history,
containing very early failures ($C_2 = 0.1$, $C_3 = 0.2$, $C_4 = 0.8$ $H = 1$, $P_2 = 0.3$, $P_3 = 0.4$).}
\label{fig:tauhist4fig3T}
\end{figure}


\section{A small simulation study}
\label{sec:sim}

%A systematic study based on simulation failure times is subsequently presented in Section~\ref{sec:sim}.
We conducted a small simulation study to illustrate the performance of our policy
as compared to corrective and age-based policies.
To be able to discern the effect of parameter learning over several operational cycles,
we simulated component failure times for five subsequent operational cycles for each simulation repetition.
We thus can compare policies that carry information gained in one operational cycle to the next operational cycle
with policies that do not update component parameters at the end of an operational cycle.
We also distinguish the condition-based maintenance policies with and without continuous parameter update
as in Section~\ref{sec:examples}.
In total, we compare the six maintenance policies listed in Table~\ref{tab:sim-abbrev},
which also gives the abbreviations used in Figures~\ref{fig:br1sim1fig5Tt01}, \ref{fig:br1sim2fig5Tt01} and \ref{fig:br1sim3fig5Tt01}.

\begin{table}
\centering
\begin{tabular}{ll}
  \toprule
Policy & Description \\
  \midrule
CBM-cpu & Condition-based maintenance (continuous parameter update)\\
CBM-epu & Condition-based maintenance (end of cycle parameter update)\\
CBM-npu & Condition-based maintenance (no parameter update)\\
ABM-epu & Age-based maintenance (end of cycle parameter update)\\
ABM-npu & Age-based maintenance (no parameter update)\\
CM      & Corrective maintenance\\
  \bottomrule
\end{tabular}
\caption{The six maintenance policies compared in the simulation study.}
\label{tab:sim-abbrev}
\end{table}

We compare these policies by three simple performance measures,
calculated for each repetition. %, which contains five operational cycles.
$\esys$ is the number of system failures that occured when applying the policy,
This is an integer between zero and five, as there are five operational cycles per repetition.
$\mrsys$ is the mean realized system runtime,
i.e., the average of the realized operational cycle length,
taken over the five operational cycles per repetition.
%(I called this 'cycle length' before),
$\bar{g}$ is the mean realized operational cycle cost rate obtained over the five operational cycles.
This is calculated as the sum of preventive and corrective maintenance costs incurred in the five operational cycles,
divided by the sum of the five realized operational cycle lengths.
%This is a weighted average of the realized operational cycle cost rate for each of the five operational cycles,
%with weights proportional to the corresponding realized operational cycle length.
%This equals 

We use again the simplified automotive brake system depicted in Figure~\ref{fig:brakesys-layout},
together with the prior parameters for the component models as given in Table~\ref{tab:priorparams},
and assume that $c_c = 1$ and $c_p = 0.2$ like before.
We study three data scenarios A, B and C, corresponding to failure times as expected, as well as
failure times earlier and later than expected.

\begin{figure}
\includegraphics[width=\textwidth]{br1sim1fig5Tt01ac}
\caption{Simulation study with 20 repetitions, where each repetition consists of 5 subsequent operational cycles.
Case A: $\lambda_k = \ykz$.}
\label{fig:br1sim1fig5Tt01}
\end{figure}

\subsection{Case A: failure times as expected, $\lambda_k = \ykz$}
\label{sec:case1}

For this scenario, we simulated component failure times according to prior expectations,
i.e., component failure times were simulated as Weibull failure times
with the scale parameter $\lambda_k$ chosen as equal to the expert's prior guess $\ykz$
for all four component types.
The results are depicted in Figure~\ref{fig:br1sim1fig5Tt01}.

Obviously, the corrective maintenance policy (CM) results in $\esys = 5$ for all 20 repetitions,
as it cannot lead to preventive maintenance.
In contrast, all condition-based (CBM) and age-based policies can avoid system failures entirely (with a single exception for ABM-epu).
Mean system runtimes are longest for CM because it runs the system until failure,
but this leads to the highest mean cost rates.
The condition-based maintenance policy with continuous parameter update (CBM-cpu) obtains the lowest mean cost rates,
since it leads to the longest system runtimes without system failures.
CBM-epu and CBM-npu have slightly higher cost rates,
as they lead to shorter system runtimes.
The age-based policy without parameter update (ABM-npu)
always schedules maintenance for $\tau_*^{(0)} = 1.6$.
As the system does not fail before this time in any of the five operational cycles within the 20 repetitions,
the mean realized system runtime is constantly $1.6$, with mean cost rate $0.2 / 1.6 = 0.125$.

\begin{figure}
\includegraphics[width=\textwidth]{br1sim2fig5Tt01ac}
\caption{Simulation study with 20 repetitions, where each repetition consists of 5 subsequent operational cycles.
Case B: $\lambda_k = 0.5\ykz$.}
\label{fig:br1sim2fig5Tt01}
\end{figure}

\subsection{Case B: failure times earlier than expected, $\lambda_k = 0.5 \ykz$}
\label{sec:case2}

For the second scenario,
component failure times were simulated as being earlier than expected,
by choosing $\lambda_k$ for all component types as half the size of the expert's prior guess,
i.e., $\lambda_k = 0.5 \ykz$ for all $k =$ C, H, M, P.
%($0.5 \times$ MTTF) than expected.
The results are depicted in Figure~\ref{fig:br1sim2fig5Tt01}.

Here, both corrective and age-based policies cannot avoid system failures entirely.
The condition-based policies result in two to three system failures in the $20 \times 5 = 100$ system runs,
whereas the age-based policies result in nine to ten system failures.
System runtimes are less variable for the age-based policies
since they do not use the current system structure information, %available information on the component level,
i.e., which components have failed and which not. %during operational cycles
System runtimes are more variable for the condition-based policies,
but longer on average, leading to lower realized cost rates.
Again, CBM-cpu performs slightly better than CBM-epu
as it has the possibility to learn within an operational cycle as well.
CBM-npu does surprisingly well, considering that it never updates prior component parameters. 
This confirms the observation from Section~\ref{sec:ex-3}
that current system structure information delivers a major part of the gains for our CBM policy.

\begin{figure}
\includegraphics[width=\textwidth]{br1sim3fig5Tt01ac}
\caption{Simulation study with 20 repetitions, where each repetition consists of 5 subsequent operational cycles.
Case C: $\lambda_k = 2\ykz$.}
\label{fig:br1sim3fig5Tt01}
\end{figure}

\subsection{Case C: failure times later than expected, $\lambda_k = 2 \ykz$}
\label{sec:case3}

The third scenario consists of component failure times taht are later than expected,
obtained by simulating Weibull lifetimes using $\lambda_k = 2 \ykz$ for all four component types.
The results are depicted in Figure~\ref{fig:br1sim3fig5Tt01}.

As the expert's information now underestimates component lifetimes,
all policies (except the corrective policy) act quite cautiously,
leading to complete avoidance of system failures.
However, the mean system runtimes $\mrsys$ exhibit
a clear hierarchy between age-based and condition-based policies,
and within these two groups a clear effect of the parameter update strategy.
Here, CBM-cpu obtains the longest system runtimes, and thus the lowest cost rates,
with a clear advantage over CBM-epu and CBM-npu.

Overall, it seems that CBM-cpu can play its adaptive strengths most effectively
when prior assumptions are too pessimistic,
while also performing well when prior assumptions are correct or too optimistic.
%This means that experts may confidently tend towards pessimistic specifications
%of the component mean time to failures $\yzk$.


% ------------- old overview figures ----------------------------------
\iffalse
\begin{figure}
\includegraphics[width=0.5\textwidth]{br1sim1fig5}
\includegraphics[width=0.5\textwidth]{br1sim2fig5}
\includegraphics[width=0.5\textwidth]{br1sim3fig5}
\includegraphics[width=0.5\textwidth]{br1sim4fig5}
\caption{***Simulation study with ***prospective costrate criterion***, $\tthresh = 0.5$.
TL: $\lambda_k =     \ykz$,
TR: $\lambda_k = 0.5 \ykz$,
BL: $\lambda_k = 2   \ykz$,
BR: $\lambda_k = 0.2 \ykz$}
\label{fig:simprospcostrate05}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{br1sim1fig5T}
\includegraphics[width=0.5\textwidth]{br1sim2fig5T}
\includegraphics[width=0.5\textwidth]{br1sim3fig5T}
\includegraphics[width=0.5\textwidth]{br1sim4fig5T}
\caption{***Simulation study with ***total costrate criterion***, $\tthresh = 0.5$.
TL: $\lambda_k =     \ykz$,
TR: $\lambda_k = 0.5 \ykz$,
BL: $\lambda_k = 2   \ykz$,
BR: $\lambda_k = 0.2 \ykz$}
\label{fig:simtotalcostrate05}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{br1sim2fig5Tt01}
\includegraphics[width=0.5\textwidth]{br1sim2fig5Tt02}
\caption{***Simulation study with ***total costrate criterion***, early failures ($0.5 \times$ MTTF),
$\tthresh = 0.1$ (left) and $\tthresh = 0.2$ (right).}
\label{fig:simtotalcostrate0102}
\end{figure}
\fi


\section{Summary and outlook}
\label{sec:outlook}

In this paper, we have proposed a new condition-based maintenance policy
for systems based on monitoring of the functioning of components.
We use the survival signature to compute system reliability functions
for given component models, and propose to use Weibull component models
with conjugate priors to express expert knowledge.
The resulting policy shows promising results in
different numerical exmples and a small simulation study.

There are several aspects in our modelling that warrant further investigation.
So far, we assume that component failure times are observed precisely via monitoring.
However, in many applications, the component status can only be determined
at inspections, leading to interval-censored failure time observations.
Extending the model to account for interval-censored failure times
would allow to derive a condition-based maintenance policy with wider-spaced inspections,
which could also determine the optimal moment for the next inspection of the system.
%*** extend the model for inspection-based CBM (if no repair necessary, find optimal timing of next inspection),
%we would need to add interval-censored data (failure has happened between last and current inspection)

As mentioned at the end of Section~\ref{sec:costrate},
we currently disregard the possibility of common cause failures,
i.e., simultaneous failures with a shared root cause.
Common cause failures seriously reduce the system reliability,
and can lead to sudden system failures.
To offer a realistic estimation of the optimal time to maintenance $\tausnow$,
the system RLD $\Rsys(t)$ needs to account for such failures when they are possible.
%This needs to be implemented on the level of component models,
%specifically $P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big)$ 
%(not a product anymore if common-case failures can affect several component types at once).
When common cause failures can affect several component types at once,
the joint posterior predictive probabilities $P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big)$ in \eqref{eq:sysrel-survsign}
cannot be obtained as a product of component posterior predictive probabilities any more.
We think that by combining ideas from \cite{2015:coolen-coolen-maturi} and \cite{Troffaes2014a},
the proposed policy could be extended to accomodate common cause failures.

In our policy, we assumed that in both preventive and corrective repair
all components are restored to an as good as new state.
An obvious alternative is to only replace the failed components,
but would this offer any advantage?
Furthermore, could it be even optimal to repair a subset of the failed components only?
Such selective component replacement schemes
seem to be a promising area for research.
To account for different ages of components at system startup,
one could introduce artificial component types in \eqref{eq:sysrel-survsign},
grouping components both by type and by age.
However, computational complexity increases with the number of component types,
%and the conceptual algorithmic advantage of the survival signature vanishes
%when each component forms its own type
so it seems most promising to replace batches of components,
or all components of one type.
%*** study other replacement schemes, e.g., replacing only failed components:
%when repaired/replaced components are present in system,
%then one needs to use actual ages (and not time since system startup) in parameter update,
%introduces new component type for $\Rsysnow(t)$ calculation but with same posterior parameters as unreplaced component type.
%Repaired/replaced components require the creation of a separate type in the survival signature decomposition
%and thus the calculation of separate posterior predictive probabilities
%$P(C^k_t = l_k \mid \nkz,\ykz,\vectknow)$,
%which use, however, the same hyperparameter learning (using actual component ages) as the `parent type'.

Another promising extension would be to use sets of conjugate priors for the component models.
This approach is described in \cite{2016:walter-coolen},
leading to sets of current residual life distributions $\Rsys(t)$.
Consequently, we would obtain a set of expected operational cycle cost rate functions $\gnow(\tau)$.
The determination of the (set of) optimal time(s) to maintenance $\tausnow$
is then non-trivial,
and would involve decision criteria discussed in the imprecise probability literature,
like E-admissibility, maximality, or maximin \citep[see, e.g., \S 8][]{itip}.


\section*{Acknowledgements}

Gero Walter was supported by the DINALOG project CAMPI
(``Coordinated Advanced Maintenance and Logistics Planning for the Process Industries'').

Both authors would like to thank Frank Coolen for inspiring discussions
and valuable hints on maintenance optimization criteria.

\section*{Bibliography}

\bibliographystyle{elsarticle-harv}

\bibliography{refs}

\end{document}
